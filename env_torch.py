"""Torch Gym Environment that interfaces with the GPU Drive simulator."""

from gymnasium.spaces import Box, Discrete, Tuple
import numpy as np
import torch
from itertools import product
import mediapy as media
import gymnasium
import random
import madrona_gpudrive
from gpudrive.datatypes.observation import (
    LocalEgoState,
    GlobalEgoState,
    PartnerObs,
    LidarObs,
    BevObs,
)

from gpudrive.env import constants
from gpudrive.env.config import EnvConfig, RenderConfig
from gpudrive.env.base_env import GPUDriveGymEnv
from gpudrive.datatypes.trajectory import LogTrajectory
from gpudrive.datatypes.roadgraph import (
    LocalRoadGraphPoints,
    GlobalRoadGraphPoints,
)
from gpudrive.datatypes.metadata import Metadata
from gpudrive.datatypes.info import Info

from gpudrive.visualize.core import MatplotlibVisualizer
from gpudrive.visualize.utils import img_from_fig

from gpudrive.utils.geometry import normalize_min_max

from gpudrive.integrations.vbd.data_utils import process_scenario_data


class GPUDriveTorchEnv(GPUDriveGymEnv):
    """Torch Gym Environment that interfaces with the GPU Drive simulator."""

    def __init__(
        self,
        config,
        sim_scenes: list, 
        max_cont_agents,
        device="cuda",
        action_type="discrete",
        render_config: RenderConfig = RenderConfig(),
        backend="torch",
    ):
        # Initialization of environment configurations
        self.config = config
        
        # [ä¿®æ”¹ 3] num_worlds ä»åœºæ™¯åˆ—è¡¨çš„é•¿åº¦æ¨å¯¼
        self.num_worlds = len(sim_scenes)
        
        # [ä¿®æ”¹ 4] data_batch ä¸å†ç”¨äºåŠ è½½åœ°å›¾ï¼Œè®¾ä¸º None
        self.data_batch = None
        
        self.max_cont_agents = max_cont_agents
        self.device = device
        self.render_config = render_config
        self.backend = backend
        self.max_num_agents_in_scene = self.config.max_num_agents_in_scene
        self.world_time_steps = torch.zeros(
            self.num_worlds, dtype=torch.short, device=self.device
        )

        # Initialize reward weights tensor if using reward_conditioned
        self.reward_weights_tensor = None
        if (
            hasattr(self.config, "reward_type")
            and self.config.reward_type == "reward_conditioned"
        ):
            condition_mode = getattr(self.config, "condition_mode", "random")
            agent_type = getattr(self.config, "agent_type", None)
            self._set_reward_weights(
                condition_mode=condition_mode, agent_type=agent_type
            )

        # Environment parameter setup
        params = self._setup_environment_parameters()
        params.maxNumControlledAgents = int(self.max_cont_agents)

        # =============================================================
        # [GIGAFLOW FIX] RewardParams åˆå§‹åŒ– - å¿…é¡»ä¸ C++ types.hpp å®Œå…¨ä¸€è‡´
        # =============================================================
        reward_params = madrona_gpudrive.RewardParams()

        # 1. è®¾ç½®å¥–åŠ±ç±»å‹
        if hasattr(self.config, "reward_type") and self.config.reward_type == "dense":
            reward_params.rewardType = madrona_gpudrive.RewardType.Dense
        elif hasattr(self.config, "reward_type") and self.config.reward_type == "sparse":
            reward_params.rewardType = madrona_gpudrive.RewardType.OnGoalAchieved
        else:
            reward_params.rewardType = madrona_gpudrive.RewardType.DistanceBased

        # 2. æ³¨å…¥ç°æœ‰æƒé‡
        reward_params.distanceToGoalThreshold = getattr(self.config, "dist_to_goal_threshold", 1.0)
        reward_params.distanceToExpertThreshold = getattr(self.config, "dist_to_expert_threshold", 3.0) # ç¡®ä¿æœ‰è¿™ä¸ª
        reward_params.rewardWeightProgress    = getattr(self.config, "reward_weight_progress", 0.05)
        reward_params.rewardWeightGoal        = getattr(self.config, "reward_weight_goal", 10.0)
        reward_params.rewardWeightCollision   = getattr(self.config, "reward_weight_collision", -10.0)
        reward_params.rewardWeightOffRoad     = getattr(self.config, "reward_weight_off_road", -5.0)
        reward_params.rewardWeightStill       = getattr(self.config, "reward_weight_still", 0.0)

        # 3. [CRITICAL FIX] æ³¨å…¥æ–°å¢æƒé‡ï¼Œä¿®å¤å†…å­˜åç§»
        # è¿™é‡Œå¯¹åº”æ‚¨ types.hpp ä¸­æ–°å¢çš„ float rewardWeightGoalDist å’Œ rewardWeightSpeed
        # å¦‚æœ config ä¸­æ²¡æœ‰é…ç½®ï¼Œç»™äºˆé»˜è®¤å€¼ 0.0
        reward_params.rewardWeightGoalDist    = getattr(self.config, "reward_weight_goal_dist", 0.0)
        reward_params.rewardWeightSpeed       = getattr(self.config, "reward_weight_speed", 0.0)

        # =============================================================

        # 4. å°† RewardParams èµ‹å€¼ç»™ä¸»å‚æ•°å¯¹è±¡
        params.rewardParams = reward_params
        
        # [ä¿®æ”¹ 5] å­˜å‚¨ä¼ å…¥çš„åœºæ™¯åˆ—è¡¨
        self.sim_scenes = sim_scenes
        print(f"\n[PYTHON PROBE] Initializing Manager...")
        print(f"  > Sending max_cont_agents: {self.max_cont_agents}")
        print(f"  > Sending rewardWeightGoalDist: {reward_params.rewardWeightGoalDist}")
        print(f"  > Sending rewardWeightSpeed: {reward_params.rewardWeightSpeed}")
        # Initialize simulator
        # [ä¿®æ”¹ 6] ä¼ é€’ self.data_batch (ä¸º None) å’Œ self.sim_scenes
        self.sim = self._initialize_simulator(params, self.data_batch, self.sim_scenes)

        # Controlled agents setup
        self.cont_agent_mask = self.get_controlled_agents_mask()
        self.max_agent_count = self.cont_agent_mask.shape[1]
        self.num_valid_controlled_agents_across_worlds = (
            self.cont_agent_mask.sum().item()
        )

        self.episode_len = self.config.episode_len

        self.partner_obs_dim = 0
        self.road_map_obs_dim = 0 
        self.bev_obs_dim = 0

        # Initialize VBD model if used
        self._initialize_vbd()

        # Setup action and observation spaces
        low, high = (-1.0, 1.0) if self.config.norm_obs else (-np.inf, np.inf)
        self.observation_space = Box(
            low=low,
            high=high,
            shape=(self.get_obs(self.cont_agent_mask).shape[-1],),
        )

        self.single_observation_space = gymnasium.spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(self.observation_space.shape[-1],),
            dtype=np.float32,
        )

        self._setup_action_space(action_type)
        self.single_action_space = self.action_space

        self.num_agents = self.cont_agent_mask.sum().item()

        # Rendering setup
        self.vis = MatplotlibVisualizer(
            sim_object=self.sim,
            controlled_agent_mask=self.cont_agent_mask,
            goal_radius=self.config.dist_to_goal_threshold,
            backend=self.backend,
            num_worlds=self.num_worlds,
            render_config=self.render_config,
            env_config=self.config,
        )

    def _update_info_dict(self):
        """æ‰‹åŠ¨æ›´æ–° info_dict ä¾› Callback è¯»å–"""
        infos = self.sim.info_tensor().to_torch() # è·å–åŸå§‹æ•°æ®
        mask = self.cont_agent_mask
        
        # å¼ºåˆ¶è½¬æ¢ä¸º floatï¼Œé¿å…æ—¥å¿—ç³»ç»Ÿæ— æ³•è¯†åˆ« int
        self.info_dict = {
            "off_road": infos[:, :, 5][mask].sum().float().item(),       # Index 5
            "veh_collisions": infos[:, :, 1:3].sum(dim=2)[mask].sum().float().item(),
            "goal_achieved": infos[:, :, 3][mask].sum().float().item(),
            "num_controlled_agents": mask.sum().item()
        }

# ... (ä» _initialize_vbd åˆ° get_expert_actions çš„æ‰€æœ‰ä»£ç ä¿æŒä¸å˜) ...
# ... (è¿™äº›å‡½æ•°ä¸ä½¿ç”¨ data_batch) ...

    def _initialize_vbd(self):
        """
        Initialize the Versatile Behavior Diffusion (VBD) model and related
        components. Link: https://arxiv.org/abs/2404.02524.

        Args:
            config: Configuration object containing VBD settings.
        """
        self.use_vbd = self.config.use_vbd
        self.vbd_trajectory_weight = self.config.vbd_trajectory_weight

        # Set initialization steps - ensure minimum steps for VBD
        if self.use_vbd:
            self.init_steps = max(
                self.config.init_steps, 10
            )  # Minimum 10 steps for VBD
        else:
            self.init_steps = self.config.init_steps

        if (
            self.use_vbd
            and hasattr(self.config, "vbd_model_path")
            and self.config.vbd_model_path
        ):
            self.vbd_model = self._load_vbd_model(self.config.vbd_model_path)

            self.vbd_trajectories = torch.zeros(
                (
                    self.num_worlds,
                    self.max_agent_count,
                    self.episode_len - self.init_steps,
                    5,
                ),
                device=self.device,
                dtype=torch.float32,
            )

            self._generate_vbd_trajectories()
        else:
            self.vbd_model = None
            self.vbd_trajectories = None

    def _load_vbd_model(self, model_path):
        """Load the Versatile Behavior Diffusion (VBD) model from checkpoint."""
        from gpudrive.integrations.vbd.sim_agent.sim_actor import VBDTest

        model = VBDTest.load_from_checkpoint(
            model_path, torch.device(self.device)
        )
        _ = model.eval()
        return model

    # gpudrive/env/env_torch.py -> _generate_sample_batch å‡½æ•° (æ›¿æ¢åŸå‡½æ•°ä½“)

    def _generate_sample_batch(self, init_steps=10):
        """Generate a sample batch for the VBD model."""
        means_xy = (
            self.sim.world_means_tensor().to_torch()[:, :2].to(self.device)
        )

        # Get the logged trajectory and restore the mean
        log_trajectory = LogTrajectory.from_tensor(
            self.sim.expert_trajectory_tensor(),
            self.num_worlds,
            self.max_agent_count,
            backend=self.backend,
        )
        log_trajectory.restore_mean(
            mean_x=means_xy[:, 0], mean_y=means_xy[:, 1]
        )

        # [GIGAFLOW FIX] æ›¿æ¢å¯¹ map_observation_tensor çš„ä¾èµ–ï¼Œä¼ é€’ä¸€ä¸ªç©ºå¯¹è±¡
        global_road_graph = GlobalRoadGraphPoints(data=torch.Tensor())
        
        # Get global agent observations and restore the mean
        global_agent_obs = GlobalEgoState.from_tensor(
            abs_self_obs_tensor=self.sim.absolute_self_observation_tensor(),
            backend=self.backend,
            device=self.device,
        )
        global_agent_obs.restore_mean(
            mean_x=means_xy[:, 0], mean_y=means_xy[:, 1]
        )
        metadata = Metadata.from_tensor(
            metadata_tensor=self.sim.metadata_tensor(),
            backend=self.backend,
        )
        sample_batch = process_scenario_data(
            max_controlled_agents=self.max_cont_agents,
            controlled_agent_mask=self.cont_agent_mask,
            global_agent_obs=global_agent_obs,
            global_road_graph=global_road_graph, # ä¼ é€’ç©ºå¯¹è±¡
            log_trajectory=log_trajectory,
            episode_len=self.episode_len,
            init_steps=init_steps,
            raw_agent_types=self.sim.info_tensor().to_torch()[:, :, 4],
            metadata=metadata,
        )
        return sample_batch
    def _set_reward_weights(
        self, env_idx_list=None, condition_mode="random", agent_type=None
    ):
        """Set agent reward weights for all or specific environments.

        Args:
            env_idx_list: List of environment indices to generate new weights for.
                          If None, all environments are updated.
            condition_mode: Determines how reward weights are sampled:
                            - "random": Random sampling within bounds (default for training)
                            - "fixed": Use predefined agent_type weights (for testing)
                            - "preset": Use a specific preset from agent_type parameter
            agent_type: Specifies which preset weights to use if condition_mode is "preset" or "fixed"
                        If condition_mode is "preset", can be one of: "cautious", "aggressive", "balanced"
                        If condition_mode is "fixed", should be a tensor of shape [3] with weight values
        """
        if self.reward_weights_tensor is None:
            self.reward_weights_tensor = torch.zeros(
                self.num_worlds,
                self.max_cont_agents,
                3,  # collision, goal_achieved, off_road
                device=self.device,
            )

        # Read bounds for the three reward components
        lower_bounds = torch.tensor(
            [
                self.config.collision_weight_lb,
                self.config.goal_achieved_weight_lb,
                self.config.off_road_weight_lb,
            ],
            device=self.device,
        )

        upper_bounds = torch.tensor(
            [
                self.config.collision_weight_ub,
                self.config.goal_achieved_weight_ub,
                self.config.off_road_weight_ub,
            ],
            device=self.device,
        )
        bounds_range = upper_bounds - lower_bounds

        # Preset agent personality types
        agent_presets = {
            "cautious": torch.tensor(
                [
                    self.config.collision_weight_lb
                    * 0.9,  # Strong collision penalty
                    self.config.goal_achieved_weight_ub
                    * 0.7,  # Moderate goal reward
                    self.config.off_road_weight_lb
                    * 0.9,  # Strong off-road penalty
                ],
                device=self.device,
            ),
            "aggressive": torch.tensor(
                [
                    self.config.collision_weight_lb
                    * 0.5,  # Lower collision penalty
                    self.config.goal_achieved_weight_ub
                    * 0.9,  # Higher goal reward
                    self.config.off_road_weight_lb
                    * 0.6,  # Moderate off-road penalty
                ],
                device=self.device,
            ),
            "balanced": torch.tensor(
                [
                    (
                        self.config.collision_weight_lb
                        + self.config.collision_weight_ub
                    )
                    / 2,
                    (
                        self.config.goal_achieved_weight_lb
                        + self.config.goal_achieved_weight_ub
                    )
                    / 2,
                    (
                        self.config.off_road_weight_lb
                        + self.config.off_road_weight_ub
                    )
                    / 2,
                ],
                device=self.device,
            ),
            "risk_taker": torch.tensor(
                [
                    self.config.collision_weight_lb
                    * 0.3,  # Minimal collision penalty
                    self.config.goal_achieved_weight_ub,  # Maximum goal reward
                    self.config.off_road_weight_lb
                    * 0.4,  # Low off-road penalty
                ],
                device=self.device,
            ),
        }

        # Determine which environments to update
        if env_idx_list is None:
            env_idx_list = list(range(self.num_worlds))

        env_indices = torch.tensor(env_idx_list, device=self.device)
        num_envs = len(env_indices)

        if condition_mode == "random":
            # Traditional random sampling within bounds
            random_values = torch.rand(
                num_envs, self.max_cont_agents, 3, device=self.device
            )
            scaled_values = lower_bounds + random_values * bounds_range

        elif condition_mode == "preset":
            # Use a predefined agent type
            if agent_type not in agent_presets:
                raise ValueError(
                    f"Unknown agent_type: {agent_type}. Available types: {list(agent_presets.keys())}"
                )

            # Create a tensor with the preset weights for all agents in the specified environments
            preset_weights = agent_presets[agent_type]
            scaled_values = (
                preset_weights.unsqueeze(0)
                .unsqueeze(0)
                .expand(num_envs, self.max_cont_agents, 3)
            )

        elif condition_mode == "fixed":
            # Use custom provided weights
            if agent_type is None or not isinstance(agent_type, torch.Tensor):
                raise ValueError(
                    "For condition_mode='fixed', agent_type must be a tensor of shape [3]"
                )

            custom_weights = agent_type.to(device=self.device)
            if custom_weights.shape != (3,):
                raise ValueError(
                    f"agent_type tensor must have shape [3], got {custom_weights.shape}"
                )

            scaled_values = (
                custom_weights.unsqueeze(0)
                .unsqueeze(0)
                .expand(num_envs, self.max_cont_agents, 3)
            )

        else:
            raise ValueError(f"Unknown condition_mode: {condition_mode}")

        # Update the weights tensor for the specified environments
        self.reward_weights_tensor[env_indices.cpu()] = scaled_values

        return self.reward_weights_tensor
    
    def _init_gigaflow_scenario(self, env_idx_list):
        """
        [GIGAFLOW ä¿®å¤] åœ¨é‡ç½®ååŒæ­¥ Python ç«¯çš„ Agent æ©ç ã€‚
        ç”±äº C++ ç¨‹åºåŒ–ç”Ÿæˆä¼šæ”¹å˜ Agent çš„æ•°é‡å’Œä½ç½®ï¼Œ
        æˆ‘ä»¬éœ€è¦æ›´æ–° cont_agent_mask ä»¥ä¾¿ Python çŸ¥é“å“ªäº› Agent æ˜¯æœ‰æ•ˆçš„ã€‚
        """
        # é‡æ–°ä» C++ è·å–å½“å‰çš„æ§åˆ¶æ©ç  (Shape: [num_worlds, max_agents])
        self.cont_agent_mask = self.get_controlled_agents_mask()
        
        # æ›´æ–°æœ‰æ•ˆçš„å—æ§ Agent æ€»æ•°
        self.num_valid_controlled_agents_across_worlds = (
            self.cont_agent_mask.sum().item()
        )

    def reset(
        self,
        mask=None,
        env_idx_list=None,
        condition_mode=None,
        agent_type=None,
    ):
        """Reset the worlds and return the initial observations.

        Args:
            mask: Optional mask indicating which agents to return observations for
            env_idx_list: Optional list of environment indices to reset
            condition_mode: Determines how reward weights are sampled:
                            - "random": Random sampling within bounds (default for training)
                            - "fixed": Use predefined agent_type weights (for testing)
                            - "preset": Use a specific preset from agent_type parameter
            agent_type: Specifies which preset weights to use or custom weights
        """
        if env_idx_list is not None:
            self.sim.reset(env_idx_list)
        else:
            env_idx_list = list(range(self.num_worlds))
            self.sim.reset(env_idx_list)

        self._init_gigaflow_scenario(env_idx_list)

        self.world_time_steps.zero_()
        # Re-initialize reward weights if using reward_conditioned
        if (
            hasattr(self.config, "reward_type")
            and self.config.reward_type == "reward_conditioned"
        ):
            # Use the specified condition_mode or default to the config setting
            mode = (
                condition_mode
                if condition_mode is not None
                else getattr(self.config, "condition_mode", "random")
            )
            self._set_reward_weights(
                env_idx_list, condition_mode=mode, agent_type=agent_type
            )

        

        return self.get_obs(mask)

    def get_dones(self):
        return (
            self.sim.done_tensor()
            .to_torch()
            .clone()
            .squeeze(dim=2)
            .to(torch.float)
        )

    def get_infos(self):
        return Info.from_tensor(
            self.sim.info_tensor(),
            backend=self.backend,
            device=self.device,
        )

    # def get_rewards(
    #     self,
    #     collision_weight=-0.5,
    #     goal_achieved_weight=1.0,
    #     off_road_weight=-0.5,
    #     world_time_steps=None,
    #     log_distance_weight=0.01,
    # ):
    #     """Obtain the rewards for the current step.
    #     By default, the reward is a weighted combination of the following components:
    #     - collision
    #     - goal_achieved
    #     - off_road

    #     The importance of each component is determined by the weights.
    #     """

    #     # Return the weighted combination of the reward components
    #     info_tensor = self.sim.info_tensor().to_torch().clone()
    #     off_road = info_tensor[:, :, 5].to(torch.float)

    #     # True if the vehicle is in collision with another road object
    #     # (i.e. a cyclist or pedestrian)
    #     collided = info_tensor[:, :, 1:3].to(torch.float).sum(axis=2)
    #     goal_achieved = info_tensor[:, :, 3].to(torch.float)

    #     if self.config.reward_type == "sparse_on_goal_achieved":
    #         return self.sim.reward_tensor().to_torch().clone().squeeze(dim=2)

    #     elif self.config.reward_type == "weighted_combination":
    #         weighted_rewards = (
    #             collision_weight * collided
    #             + goal_achieved_weight * goal_achieved
    #             + off_road_weight * off_road
    #         )

    #         return weighted_rewards

    #     elif self.config.reward_type == "reward_conditioned":
    #         # Extract individual weight components from the tensor
    #         # Shape: [num_worlds, max_agents, 3]
    #         if self.reward_weights_tensor is None:
    #             self._set_reward_weights()

    #         # Apply the weights in a vectorized manner
    #         # Each index in dimension 2 corresponds to a specific weight:
    #         # 0: collision, 1: goal_achieved, 2: off_road
    #         weighted_rewards = (
    #             self.reward_weights_tensor[:, :, 0] * collided
    #             + self.reward_weights_tensor[:, :, 1] * goal_achieved
    #             + self.reward_weights_tensor[:, :, 2] * off_road
    #         )

    #         return weighted_rewards

    #     elif self.config.reward_type == "distance_to_vdb_trajs":
    #         # Reward based on distance to VBD predicted trajectories
    #         # (i.e. the deviation from the predicted trajectory)
    #         weighted_rewards = (
    #             collision_weight * collided
    #             + goal_achieved_weight * goal_achieved
    #             + off_road_weight * off_road
    #         )

    #         agent_states = GlobalEgoState.from_tensor(
    #             self.sim.absolute_self_observation_tensor(),
    #             self.backend,
    #             self.device,
    #         )

    #         agent_pos = torch.stack(
    #             [agent_states.pos_x, agent_states.pos_y], dim=-1
    #         )

    #         # Extract VBD positions at current time steps for each world
    #         vbd_pos = []
    #         for i in range(self.num_worlds):
    #             current_time = (
    #                 self.world_time_steps[i].item() - self.init_steps
    #             )
    #             # Make sure we don't exceed trajectory length
    #             current_time = min(
    #                 current_time, self.vbd_trajectories.shape[2] - 1
    #             )
    #             vbd_pos.append(self.vbd_trajectories[i, :, current_time, :2])
    #         vbd_pos_tensor = torch.stack(vbd_pos)

    #         # Compute euclidean distance between agent and logs
    #         dist_to_vbd = torch.norm(vbd_pos_tensor - agent_pos, dim=-1)

    #         # Add reward based on inverse distance to logs
    #         weighted_rewards += self.vbd_trajectory_weight * torch.exp(
    #             -dist_to_vbd
    #         )

    #         return weighted_rewards

    #     elif self.config.reward_type == "distance_to_logs":
    #         # Reward based on distance to logs and penalty for collision
    #         weighted_rewards = (
    #             collision_weight * collided
    #             + goal_achieved_weight * goal_achieved
    #             + off_road_weight * off_road
    #         )

    #         log_trajectory = LogTrajectory.from_tensor(
    #             self.sim.expert_trajectory_tensor(),
    #             self.num_worlds,
    #             self.max_agent_count,
    #             backend=self.backend,
    #         )

    #         # Index log positions at current time steps
    #         log_traj_pos = []
    #         for i in range(self.num_worlds):
    #             log_traj_pos.append(
    #                 log_trajectory.pos_xy[i, :, world_time_steps[i], :]
    #             )
    #         log_traj_pos_tensor = torch.stack(log_traj_pos)

    #         agent_state = GlobalEgoState.from_tensor(
    #             self.sim.absolute_self_observation_tensor(),
    #             self.backend,
    #         )

    #         agent_pos = torch.stack(
    #             [agent_state.pos_x, agent_state.pos_y], dim=-1
    #         )

    #         # compute euclidean distance between agent and logs
    #         dist_to_logs = torch.norm(log_traj_pos_tensor - agent_pos, dim=-1)

    #         # add reward based on inverse distance to logs
    #         weighted_rewards += log_distance_weight * torch.exp(-dist_to_logs)

    #         return weighted_rewards
    # def get_rewards(
    #     self,
    #     collision_weight=-0.5,
    #     goal_achieved_weight=1.0,
    #     off_road_weight=-0.5,
    #     world_time_steps=None,
    #     log_distance_weight=0.01,
    # ):
    #     """Obtain the rewards for the current step.
        
    #     If reward_type is 'dense', it reads directly from the C++ backend (Recommended).
    #     Otherwise, it computes the reward in Python (Slower, but flexible).
    #     """

    #     # ============================================================
    #     # 1. [C++ è·¯å¾„] Dense Reward (æœ€é«˜æ•ˆï¼Œæ¨è)
    #     # ============================================================
    #     if hasattr(self.config, "reward_type") and self.config.reward_type == "dense":
    #         # ç›´æ¥ä» GPU æ˜¾å­˜è¯»å– C++ è®¡ç®—å¥½çš„å¥–åŠ±
    #         # C++ è¿”å›å½¢çŠ¶ (num_worlds, max_agents, 1)ï¼Œsqueeze ä¸º (num_worlds, max_agents)
    #         return self.sim.reward_tensor().to_torch().clone().squeeze(dim=2)

    #     # ============================================================
    #     # 2. [Python è·¯å¾„] å‡†å¤‡åŸºç¡€æ•°æ®
    #     # ============================================================
    #     # è·å–åŸºç¡€äº‹ä»¶ä¿¡æ¯ (ç¢°æ’ã€è¾¾æˆç›®æ ‡ã€è¶Šé‡)
    #     info_tensor = self.sim.info_tensor().to_torch().clone()
    #     off_road = info_tensor[:, :, 5].to(torch.float)
        
    #     # True if the vehicle is in collision with another road object (i.e. a cyclist or pedestrian)
    #     collided = info_tensor[:, :, 1:3].to(torch.float).sum(axis=2)
    #     goal_achieved = info_tensor[:, :, 3].to(torch.float)
    #     # ================= [è°ƒè¯•æ¢é’ˆ 1ï¼šæ£€æŸ¥æ˜¯å¦è¿›å…¥å‡½æ•°] =================
    #     if not hasattr(self, "_probe_counter"):
    #         self._probe_counter = 0
    #     self._probe_counter += 1

    #     # æ¯ 10 æ­¥å°±æ‰“å°ä¸€æ¬¡ï¼Œç¡®ä¿ä½ èƒ½ç«‹åˆ»çœ‹åˆ°
    #     if self._probe_counter % 10 == 0:
    #         print(f"[Probe] get_rewards called (Step {self._probe_counter}). Reward Type: {self.config.reward_type}", flush=True)
    #     # ===============================================================
    #     # ============================================================
    #     # 3. æ ¹æ®ç±»å‹è®¡ç®—å¥–åŠ±
    #     # ============================================================
        
    #     if self.config.reward_type == "sparse_on_goal_achieved":
    #         # ç¨€ç–å¥–åŠ± (ä» C++ è¯»å–)
    #         return self.sim.reward_tensor().to_torch().clone().squeeze(dim=2)

    #     elif self.config.reward_type == "weighted_combination":
    #         # --------------------------------------------------------
    #         # [æ–°å¢] è®¡ç®—è·ç¦»å’Œé€Ÿåº¦ (Python å®ç°)
    #         # --------------------------------------------------------
    #         # è§£æè‡ªè½¦è§‚å¯Ÿæ•°æ® (LocalEgoState)
    #         # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ç¡®ä¿ LocalEgoState å·²ç»å¯¼å…¥
    #         cpp_rewards = self.sim.reward_tensor().to_torch().squeeze(dim=2)
    #         ego_state = LocalEgoState(self.sim.self_observation_tensor().to_torch())
            
    #         # --- æ‰‹åŠ¨æå–éœ€è¦çš„æ•°æ® (é¿å… LocalEgoState åˆå§‹åŒ–å¼€é”€) ---
    #         # å‡è®¾ index 0=speed, 3=rel_goal_x, 4=rel_goal_y (éœ€ä¸ C++ è¿™é‡Œä¸€è‡´)
    #         speed = ego_state.speed
                
    #         # 1. è®¡ç®—åˆ°ç›®æ ‡çš„è·ç¦» (L2 Norm)
    #         dist_to_goal = torch.sqrt(ego_state.rel_goal_x**2 + ego_state.rel_goal_y**2)
            

    #         # 3. è·å–æƒé‡ (å°è¯•ä» config è¯»å–ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼)
    #         w_progress  = getattr(self.config, "reward_weight_progress", 0.05)
    #         w_still     = getattr(self.config, "reward_weight_still", 0.0)
    #         w_goal_dist = getattr(self.config, "reward_weight_goal_dist", 0.0) # æ–°å¢
    #         w_speed     = getattr(self.config, "reward_weight_speed", 0.0)
    #         # # ================= [æ–°å¢æ¢é’ˆ START] =================
    #         # # è¿™é‡Œçš„é€»è¾‘æ˜¯ï¼šåªåœ¨ç¬¬ 0 ä¸ª World çš„ç¬¬ 10 æ­¥æ‰“å°ä¸€æ¬¡ï¼Œé¿å…åˆ·å±
    #         # # self.sim.world_reset_tensor()[0] == 0 ç¡®ä¿åªçœ‹ World 0 (å¦‚æœå®ƒæ²¡åœ¨é‡ç½®çŠ¶æ€)
    #         # # æˆ‘ä»¬éšæœºé€‰ä¸€ä¸ªæ¦‚è§ˆæ—¶åˆ»ï¼Œæˆ–è€…ç®€å•åœ°æ¯éš” 1000 æ¬¡è°ƒç”¨æ‰“å°ä¸€æ¬¡
            
    #         # if not hasattr(self, "_probe_counter"):
    #         #     self._probe_counter = 0
    #         # self._probe_counter += 1

    #         # # æ¯ 10 æ¬¡ step æ‰“å°ä¸€æ¬¡è¯Šæ–­ä¿¡æ¯ (é¢‘ç‡å¯è°ƒ)
    #         # if self._probe_counter % 10 == 0:
    #         #     # é€‰å–å‰ 5 ä¸ªæœ‰æ•ˆ Agent çœ‹çœ‹æƒ…å†µ
    #         #     mask = self.cont_agent_mask
    #         #     valid_indices = torch.nonzero(mask.flatten(), as_tuple=True)[0][:5]
                
    #         #     print(f"\nğŸ” [Probe @ Step {self._probe_counter}] ç‰©ç†é‡æ£€æŸ¥:")
    #         #     if len(valid_indices) > 0:
    #         #         # æ‰“å°å‰å‡ ä¸ª Agent çš„å…·ä½“æ•°å€¼
    #         #         sample_speeds = speed.flatten()[valid_indices]
    #         #         sample_dists = dist_to_goal.flatten()[valid_indices]
                    
    #         #         print(f"   > é‡‡æ · Agent é€Ÿåº¦ (m/s): {sample_speeds.cpu().numpy()}")
    #         #         print(f"   > é‡‡æ · Agent ç›®æ ‡è·ç¦» (m): {sample_dists.cpu().numpy()}")
    #         #         print(f"   > å½“å‰æƒé‡: w_speed={w_speed}, w_dist={w_goal_dist}")
                    
    #         #         # æ£€æŸ¥æ˜¯å¦å…¨ 0
    #         #         if sample_speeds.abs().sum() < 1e-4:
    #         #             print("   âš ï¸ è­¦å‘Š: é‡‡æ · Agent é€Ÿåº¦å…¨ä¸º 0ï¼æ™ºèƒ½ä½“å¯èƒ½æœªå¯åŠ¨æˆ–å¡ä½ã€‚")
    #         #         if sample_dists.abs().sum() < 1e-4:
    #         #             print("   âš ï¸ è­¦å‘Š: é‡‡æ · Agent ç›®æ ‡è·ç¦»å…¨ä¸º 0ï¼å¯èƒ½å·²åˆ°è¾¾æˆ–ç›®æ ‡ç”Ÿæˆå¤±è´¥ã€‚")
    #         #     else:
    #         #         print("   âš ï¸ å½“å‰æ²¡æœ‰æ´»è·ƒçš„å—æ§ Agentã€‚")
    #         #     print("-" * 50)
    #         # # ================= [æ–°å¢æ¢é’ˆ END] ===================
    #         self.latest_reward_components = {
    #             "rew_speed": (w_speed * speed).detach(),          # é€Ÿåº¦å¥–åŠ±åˆ†é‡
    #             "rew_goal_dist": (-w_goal_dist * dist_to_goal).detach(), # è·ç¦»æƒ©ç½šåˆ†é‡ (æ³¨æ„æ˜¯è´Ÿå·ï¼Œå› ä¸ºè·ç¦»è¶Šè¿œæƒ©ç½šè¶Šå¤§)
    #             "raw_speed": speed.detach(),                      # (å¯é€‰) åŸå§‹é€Ÿåº¦ m/s
    #             "raw_dist": dist_to_goal.detach()                 # (å¯é€‰) åŸå§‹è·ç¦» m
    #         }
    #         # --------------------------------------------------------
    #         # ç»„åˆæ‰€æœ‰å¥–åŠ±é¡¹
    #         # --------------------------------------------------------
    #         weighted_rewards = (
    #             collision_weight * collided
    #             + goal_achieved_weight * goal_achieved
    #             + off_road_weight * off_road
    #             + w_speed * speed
    #             - w_goal_dist * dist_to_goal # è·ç¦»è¶Šè¿œæƒ©ç½šè¶Šå¤§
    #         )
    #         return weighted_rewards

    #     elif self.config.reward_type == "reward_conditioned":
    #         # æ¡ä»¶åŒ–å¥–åŠ± (ä½¿ç”¨ self.reward_weights_tensor)
    #         if self.reward_weights_tensor is None:
    #             self._set_reward_weights()
            
    #         # Apply the weights in a vectorized manner
    #         # Each index in dimension 2 corresponds to a specific weight:
    #         # 0: collision, 1: goal_achieved, 2: off_road
    #         weighted_rewards = (
    #             self.reward_weights_tensor[:, :, 0] * collided
    #             + self.reward_weights_tensor[:, :, 1] * goal_achieved
    #             + self.reward_weights_tensor[:, :, 2] * off_road
    #         )
    #         return weighted_rewards

    #     elif self.config.reward_type == "distance_to_vdb_trajs":
    #         # Reward based on distance to VBD predicted trajectories
    #         weighted_rewards = (
    #             collision_weight * collided
    #             + goal_achieved_weight * goal_achieved
    #             + off_road_weight * off_road
    #         )

    #         agent_states = GlobalEgoState.from_tensor(
    #             self.sim.absolute_self_observation_tensor(),
    #             self.backend,
    #             self.device,
    #         )

    #         agent_pos = torch.stack(
    #             [agent_states.pos_x, agent_states.pos_y], dim=-1
    #         )

    #         # Extract VBD positions at current time steps for each world
    #         vbd_pos = []
    #         for i in range(self.num_worlds):
    #             current_time = (
    #                 self.world_time_steps[i].item() - self.init_steps
    #             )
    #             # Make sure we don't exceed trajectory length
    #             current_time = min(
    #                 current_time, self.vbd_trajectories.shape[2] - 1
    #             )
    #             vbd_pos.append(self.vbd_trajectories[i, :, current_time, :2])
    #         vbd_pos_tensor = torch.stack(vbd_pos)

    #         # Compute euclidean distance between agent and logs
    #         dist_to_vbd = torch.norm(vbd_pos_tensor - agent_pos, dim=-1)

    #         # Add reward based on inverse distance to logs
    #         weighted_rewards += self.vbd_trajectory_weight * torch.exp(
    #             -dist_to_vbd
    #         )
    #         return weighted_rewards

    #     elif self.config.reward_type == "distance_to_logs":
    #         # Reward based on distance to logs and penalty for collision
    #         weighted_rewards = (
    #             collision_weight * collided
    #             + goal_achieved_weight * goal_achieved
    #             + off_road_weight * off_road
    #         )

    #         log_trajectory = LogTrajectory.from_tensor(
    #             self.sim.expert_trajectory_tensor(),
    #             self.num_worlds,
    #             self.max_agent_count,
    #             backend=self.backend,
    #         )

    #         # Index log positions at current time steps
    #         log_traj_pos = []
    #         for i in range(self.num_worlds):
    #             # Use passed world_time_steps if available, otherwise use self.world_time_steps
    #             ts = world_time_steps[i] if world_time_steps is not None else self.world_time_steps[i]
    #             log_traj_pos.append(
    #                 log_trajectory.pos_xy[i, :, ts, :]
    #             )
    #         log_traj_pos_tensor = torch.stack(log_traj_pos)

    #         agent_state = GlobalEgoState.from_tensor(
    #             self.sim.absolute_self_observation_tensor(),
    #             self.backend,
    #         )

    #         agent_pos = torch.stack(
    #             [agent_state.pos_x, agent_state.pos_y], dim=-1
    #         )

    #         # compute euclidean distance between agent and logs
    #         dist_to_logs = torch.norm(log_traj_pos_tensor - agent_pos, dim=-1)

    #         # add reward based on inverse distance to logs
    #         weighted_rewards += log_distance_weight * torch.exp(-dist_to_logs)

    #         return weighted_rewards
        
    #     else:
    #         # Fallback for unknown reward types
    def get_rewards(
        self,
        collision_weight=-0.5,
        goal_achieved_weight=1.0,
        off_road_weight=-0.5,
        world_time_steps=None,
        log_distance_weight=0.01,
    ):
        """Obtain the rewards for the current step.
        
        Hybrid Approach:
        - Reads physical penalties (off-road, collision) from C++ backend.
        - Adds shaping rewards (speed, distance) from Python calculation.
        """
        if not hasattr(self, "_global_probe_printed"):
            print(f"\n[CRITICAL DEBUG] get_rewards called!")
            print(f"  > Config Reward Type: '{self.config.reward_type}'")
            self._global_probe_printed = True
        # ============================================================
        # 1. [çº¯ C++ è·¯å¾„] Dense Reward (å¦‚æœä¸æƒ³è¦ Python å¡‘å½¢ï¼Œå¯ç”¨æ­¤æ¨¡å¼)
        # ============================================================
        if hasattr(self.config, "reward_type") and self.config.reward_type == "dense":
            return self.sim.reward_tensor().to_torch().clone().squeeze(dim=2)

        # ============================================================
        # 2. [æ··åˆè·¯å¾„] Weighted Combination (C++ æƒ©ç½š + Python å¡‘å½¢)
        # ============================================================
        elif self.config.reward_type == "weighted_combination":
            
            # --- A. è·å– C++ è®¡ç®—çš„åŸºç¡€å¥–åŠ± ---
            # åŒ…å«äº†ä½ åœ¨ sim.cpp ä¸­å†™çš„:
            #   ctx.get<Reward> -= 10.0 (å®Œå…¨è¶Šé‡)
            #   ctx.get<Reward> -= 0.05 (éƒ¨åˆ†è¶Šé‡)
            #   (ä»¥åŠ C++ å¯èƒ½è®¡ç®—çš„ collision æƒ©ç½š)
            cpp_rewards = self.sim.reward_tensor().to_torch().clone().squeeze(dim=2)

            # --- B. è·å– Python ç«¯æ‰€éœ€çš„çŠ¶æ€æ•°æ® ---
            # 1. åŸºç¡€äº‹ä»¶æ ‡å¿—
            info_tensor = self.sim.info_tensor().to_torch()
            goal_achieved = info_tensor[:, :, 3].to(torch.float)
            
            # 2. ç‰©ç†çŠ¶æ€ (é€Ÿåº¦ & ä½ç½®)
            # ç›´æ¥è¯»å– self_observation_tensor (é¿å… LocalEgoState é¢å¤–å¼€é”€)
            # å¸ƒå±€: [Speed, Len, Wid, RelGoalX, RelGoalY, ...]
            self_obs = self.sim.self_observation_tensor().to_torch()
            speed = self_obs[:, :, 0]
            rel_goal_x = self_obs[:, :, 3]
            rel_goal_y = self_obs[:, :, 4]
            
            # è®¡ç®—è·ç¦» (L2 Norm)
            dist_to_goal = torch.sqrt(rel_goal_x**2 + rel_goal_y**2)

            # --- C. è¯»å–æƒé‡é…ç½® ---
            # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬æ•…æ„å¿½ç•¥ off_road_weightï¼Œå› ä¸º C++ å·²ç»ç®—è¿‡äº†
            w_goal      = getattr(self.config, "reward_weight_goal", 10.0)
            w_speed     = getattr(self.config, "reward_weight_speed", 0.0)
            w_goal_dist = getattr(self.config, "reward_weight_goal_dist", 0.01)

            self.latest_reward_components = {
                "rew_speed": (w_speed * speed).detach(),
                "rew_goal_dist": (-w_goal_dist * dist_to_goal).detach(),
            }

            # Total = C++åŸºå‡†(å«æƒ©ç½š) + åˆ°è¾¾å¥–åŠ± + é€Ÿåº¦å¥–åŠ± - è·ç¦»æƒ©ç½š
            total_reward = (
                cpp_rewards 
                + (w_goal * goal_achieved) 
                + (w_speed * speed) 
                - (w_goal_dist * dist_to_goal)
            )

            # [è¯Šæ–­æ¢é’ˆ] æ£€æŸ¥æ··åˆæ˜¯å¦ç”Ÿæ•ˆ (ä»…æ‰“å°ä¸€æ¬¡)
            # æ–‡ä»¶: gpudrive/env/env_torch.py
# ä½ç½®: get_rewards å‡½æ•°å†…éƒ¨ï¼Œè®¡ç®— cpp_rewards ä¹‹å

# ...
            # --- [ä¿®æ”¹å‰] åŸæ¥çš„ä»£ç  (å¯èƒ½è¢«æ³¨é‡Šæ‰äº†) ---
            # print(f"   > C++ Base (Mean): {cpp_rewards.mean().item():.4f} ...")

            # --- [ä¿®æ”¹å] æ–°çš„æ‰“å°é€»è¾‘ (è¿‡æ»¤ Padding) ---
            if not hasattr(self, "_hybrid_probe_printed"):
                print(f"\nâœ… [Reward System] Hybrid Mode Activated:")
                
                # 1. è·å–åŸºç¡€æ©ç 
                mask = self.cont_agent_mask
                
                # 2. [æ–°å¢] å¹½çµè¿‡æ»¤å™¨
                # æ­£å¸¸çš„ç‰©ç†å¥–åŠ±ç»ä¸ä¼šä½äº -500 (é™¤éé£å‡ºåœ°çƒ)
                # å¹½çµè½¦åœ¨ -11000 ä½ç½®ï¼Œå¥–åŠ±é€šå¸¸æ˜¯ -10000 å·¦å³
                is_valid_physics = (cpp_rewards > -500.0)
                
                # 3. ç»„åˆæ©ç ï¼šæ—¢è¦æ˜¯å—æ§çš„ï¼Œç‰©ç†çŠ¶æ€ä¹Ÿå¿…é¡»æ­£å¸¸
                clean_mask = mask & is_valid_physics
                
                # 4. è®¡ç®—å¹¶æ‰“å°â€œæ¸…æ´—åâ€çš„çœŸå€¼
                if clean_mask.sum() > 0:
                    clean_mean = cpp_rewards[clean_mask].mean().item()
                    print(f"   > C++ Base (Active & Cleaned): {clean_mean:.4f} (True Physics Reward)")
                    print(f"     (Based on {clean_mask.sum()} valid agents)")
                else:
                    print(f"   > C++ Base: No Valid Agents Found!")

                # 5. è¯Šæ–­è¢«è¿‡æ»¤æ‰çš„å¹½çµ
                ghost_count = (mask & ~is_valid_physics).sum().item()
                if ghost_count > 0:
                    print(f"   âš ï¸ WARNING: Filtered out {ghost_count} 'Ghost' agents (Reward < -500).")
                    print(f"      This confirms map coordinates are fixed, but agent count sync has issues.")

                self._hybrid_probe_printed = True
# ...
           

            return total_reward

        # ============================================================
        # 3. å…¶ä»–æ—§æ¨¡å¼ (ä¿æŒå…¼å®¹æ€§)
        # ============================================================
        elif self.config.reward_type == "sparse_on_goal_achieved":
            return self.sim.reward_tensor().to_torch().clone().squeeze(dim=2)

        elif self.config.reward_type == "reward_conditioned":
            if self.reward_weights_tensor is None:
                self._set_reward_weights()
            
            # è·å–åŸºç¡€äº‹ä»¶
            info_tensor = self.sim.info_tensor().to_torch()
            collided = info_tensor[:, :, 1:3].sum(axis=2).to(torch.float)
            goal_achieved = info_tensor[:, :, 3].to(torch.float)
            off_road = info_tensor[:, :, 5].to(torch.float)

            weighted_rewards = (
                self.reward_weights_tensor[:, :, 0] * collided
                + self.reward_weights_tensor[:, :, 1] * goal_achieved
                + self.reward_weights_tensor[:, :, 2] * off_road
            )
            return weighted_rewards

        elif self.config.reward_type == "distance_to_vdb_trajs":
            # ... (ä¿æŒåŸæœ‰çš„ VBD é€»è¾‘ä¸å˜) ...
            # ä¸ºäº†èŠ‚çœç¯‡å¹…ï¼Œè¿™é‡Œå‡è®¾åŸæœ‰é€»è¾‘ä¿æŒä¸å˜
            # å¦‚æœä½ éœ€è¦è¿™éƒ¨åˆ†ä»£ç ï¼Œè¯·å‘Šè¯‰æˆ‘ï¼Œæˆ‘å†è¡¥å…¨
            pass 
            
        elif self.config.reward_type == "distance_to_logs":
             # ... (ä¿æŒåŸæœ‰çš„ Log è·ç¦»é€»è¾‘ä¸å˜) ...
             pass

        # é»˜è®¤å›é€€
        return torch.zeros_like(self.sim.reward_tensor().to_torch().squeeze(dim=2))

            
    def step_dynamics(self, actions):
        if actions is not None:
            self._apply_actions(actions)
        self.sim.step()
        # è·å–è§‚æµ‹æ•°æ®
        obs_check = self.sim.lidar_tensor().to_torch()
        
        # æ£€æŸ¥æœ€å°å€¼
        min_val = obs_check.min().item()
        
        # é˜ˆå€¼è®¾ä¸º -200 (æ­£å¸¸å½’ä¸€åŒ–æ•°æ®é€šå¸¸åœ¨ -1 åˆ° 1 ä¹‹é—´ï¼Œç‰©ç†åæ ‡ä¹Ÿå°±å‡ ç™¾ç±³)
        # å¹½çµåæ ‡é€šå¸¸æ˜¯ -11000
        if min_val < -2000.0: 
            print(f"\nğŸš¨ [CRITICAL ALERT] è®­ç»ƒæ•°æ®ä¸­æ£€æµ‹åˆ°å¹½çµæ™ºèƒ½ä½“ï¼")
            print(f"   > Min Value Found: {min_val}")
            print(f"   > è¿™æ„å‘³ç€ C++ ç«¯çš„ reconstructLogic ä¿®å¤æœªç”Ÿæ•ˆæˆ–æœªè¢«è°ƒç”¨ã€‚")
        
        self._update_info_dict()
        not_done_worlds = ~self.get_dones().any(
            dim=1
        )  # Check if any agent in world is done
        self.world_time_steps[not_done_worlds] += 1

    def _apply_actions(self, actions):
        """Apply the actions to the simulator."""

        if (
            self.config.dynamics_model == "classic"
            or self.config.dynamics_model == "bicycle"
            or self.config.dynamics_model == "delta_local"
        ):
            if actions.dim() == 2:  # (num_worlds, max_agent_count)
                # Map action indices to action values if indices are provided
                actions = (
                    torch.nan_to_num(actions, nan=0).long().to(self.device)
                )
                action_value_tensor = self.action_keys_tensor[actions]

            elif actions.dim() == 3:
                if actions.shape[2] == 1:
                    actions = actions.squeeze(dim=2).to(self.device)
                    action_value_tensor = self.action_keys_tensor[actions]
                else:  # Assuming we are given the actual action values
                    action_value_tensor = actions.to(self.device)
            else:
                raise ValueError(f"Invalid action shape: {actions.shape}")

        else:
            action_value_tensor = actions.to(self.device)

        # Feed the action values to gpudrive
        self._copy_actions_to_simulator(action_value_tensor)

    def _copy_actions_to_simulator(self, actions):
        """Copy the provided actions to the simulator."""
        if (
            self.config.dynamics_model == "classic"
            or self.config.dynamics_model == "bicycle"
        ):
            # Action space: (acceleration, steering, heading)
            self.sim.action_tensor().to_torch()[:, :, :3].copy_(actions)
        elif self.config.dynamics_model == "delta_local":
            # Action space: (dx, dy, dyaw)
            self.sim.action_tensor().to_torch()[:, :, :3].copy_(actions)
        elif self.config.dynamics_model == "state":
            # Following the StateAction struct in types.hpp
            # Need to provide:
            # (x, y, z, yaw, vel x, vel y, vel z, ang_vel_x, ang_vel_y, ang_vel_z)
            self.sim.action_tensor().to_torch()[:, :, :10].copy_(actions)
        else:
            raise ValueError(
                f"Invalid dynamics model: {self.config.dynamics_model}"
            )

    def _set_discrete_action_space(self) -> None:
        """Configure the discrete action space based on dynamics model."""
        products = None

        if self.config.dynamics_model == "delta_local":
            self.dx = self.config.dx.to(self.device)
            self.dy = self.config.dy.to(self.device)
            self.dyaw = self.config.dyaw.to(self.device)
            products = product(self.dx, self.dy, self.dyaw)
        elif (
            self.config.dynamics_model == "classic"
            or self.config.dynamics_model == "bicycle"
        ):
            self.steer_actions = self.config.steer_actions.to(self.device)
            self.accel_actions = self.config.accel_actions.to(self.device)
            self.head_actions = self.config.head_tilt_actions.to(self.device)
            products = product(
                self.accel_actions, self.steer_actions, self.head_actions
            )
        elif self.config.dynamics_model == "state":
            self.x = self.config.x.to(self.device)
            self.y = self.config.y.to(self.device)
            self.yaw = self.config.yaw.to(self.device)
            self.vx = self.config.vx.to(self.device)
            self.vy = self.config.vy.to(self.device)

        else:
            raise ValueError(
                f"Invalid dynamics model: {self.config.dynamics_model}"
            )

        # Create a mapping from action indices to action values
        self.action_key_to_values = {}
        self.values_to_action_key = {}
        if products is not None:
            for action_idx, (action_1, action_2, action_3) in enumerate(
                products
            ):
                self.action_key_to_values[action_idx] = [
                    action_1.item(),
                    action_2.item(),
                    action_3.item(),
                ]
                self.values_to_action_key[
                    round(action_1.item(), 5),
                    round(action_2.item(), 5),
                    round(action_3.item(), 5),
                ] = action_idx

            self.action_keys_tensor = torch.tensor(
                [
                    self.action_key_to_values[key]
                    for key in sorted(self.action_key_to_values.keys())
                ]
            ).to(self.device)

            return Discrete(n=int(len(self.action_key_to_values)))
        else:
            return Discrete(n=1)

    def _set_continuous_action_space(self) -> None:
        """Configure the continuous action space."""
        if self.config.dynamics_model == "delta_local":
            self.dx = self.config.dx.to(self.device)
            self.dy = self.config.dy.to(self.device)
            self.dyaw = self.config.dyaw.to(self.device)
            action_1 = self.dx.clone().cpu().numpy()
            action_2 = self.dy.clone().cpu().numpy()
            action_3 = self.dyaw.clone().cpu().numpy()
        elif self.config.dynamics_model == "classic":
            self.steer_actions = self.config.steer_actions.to(self.device)
            self.accel_actions = self.config.accel_actions.to(self.device)
            self.head_actions = torch.tensor([0], device=self.device)
            action_1 = self.steer_actions.clone().cpu().numpy()
            action_2 = self.accel_actions.clone().cpu().numpy()
            action_3 = self.head_actions.clone().cpu().numpy()
        else:
            raise ValueError(
                f"Continuous action space is currently not supported for dynamics_model: {self.config.dynamics_model}."
            )

        action_space = Tuple(
            (
                Box(action_1.min(), action_1.max(), shape=(1,)),
                Box(action_2.min(), action_2.max(), shape=(1,)),
                Box(action_3.min(), action_3.max(), shape=(1,)),
            )
        )
        return action_space

    def _get_ego_state(self, mask=None) -> torch.Tensor:
        """Get the ego state."""

        if not self.config.ego_state:
            return torch.Tensor().to(self.device)

        ego_state = LocalEgoState.from_tensor(
            self_obs_tensor=self.sim.self_observation_tensor(),
            backend=self.backend,
            device=self.device,
            mask=mask,
        )
        if self.config.norm_obs:
            ego_state.normalize()

        if mask is None:
            if self.config.reward_type == "reward_conditioned":
                return torch.stack(
                    [
                        ego_state.speed,
                        ego_state.vehicle_length,
                        ego_state.vehicle_width,
                        ego_state.rel_goal_x,
                        ego_state.rel_goal_y,
                        ego_state.is_collided,
                        self.reward_weights_tensor[:, :, 0],
                        self.reward_weights_tensor[:, :, 1],
                        self.reward_weights_tensor[:, :, 2],
                    ]
                ).permute(1, 2, 0)

            else:
                return torch.stack(
                    [
                        ego_state.speed,
                        ego_state.vehicle_length,
                        ego_state.vehicle_width,
                        ego_state.rel_goal_x,
                        ego_state.rel_goal_y,
                        ego_state.is_collided,
                    ]
                ).permute(1, 2, 0)

        else:
            if self.config.reward_type == "reward_conditioned":
                return torch.stack(
                    [
                        ego_state.speed,
                        ego_state.vehicle_length,
                        ego_state.vehicle_width,
                        ego_state.rel_goal_x,
                        ego_state.rel_goal_y,
                        ego_state.is_collided,
                        self.reward_weights_tensor[mask][:, 0],
                        self.reward_weights_tensor[mask][:, 1],
                        self.reward_weights_tensor[mask][:, 2],
                    ]
                ).permute(1, 0)
            else:
                return torch.stack(
                    [
                        ego_state.speed,
                        ego_state.vehicle_length,
                        ego_state.vehicle_width,
                        ego_state.rel_goal_x,
                        ego_state.rel_goal_y,
                        ego_state.is_collided,
                    ]
                ).permute(1, 0)

    # def _get_partner_obs(self, mask=None):
    #     """Get partner observations."""

    #     if not self.config.partner_obs:
    #         return torch.Tensor().to(self.device)

    #     partner_obs = PartnerObs.from_tensor(
    #         partner_obs_tensor=self.sim.partner_observations_tensor(),
    #         backend=self.backend,
    #         device=self.device,
    #         mask=mask,
    #     )

    #     

    #     if mask is not None:
    #         return partner_obs.data.flatten(start_dim=1)
    #     else:
    #         return torch.concat(
    #             [
    #                 partner_obs.speed,
    #                 partner_obs.rel_pos_x,
    #                 partner_obs.rel_pos_y,
    #                 partner_obs.orientation,
    #                 partner_obs.vehicle_length,
    #                 partner_obs.vehicle_width,
    #             ],
    #             dim=-1,
    #         ).flatten(start_dim=2)

    # æ–‡ä»¶: gpudrive/env/env_torch.py




    # æ–‡ä»¶: gpudrive/env/env_torch.py

    def _get_partner_obs(self, mask=None):
        """
        [GIGAFLOW FINAL] Get partner observations with auto-reshape and flattening.
        """
        if hasattr(self.config, "partner_obs") and not self.config.partner_obs:
            return torch.Tensor().to(self.device)

        # 1. è·å–åŸå§‹ Tensor (N, A, K*9) æˆ– (N, A, K, 9)
        obs_tensor = self.sim.partner_observations_tensor().to_torch()
        
        # [AUTO-FIX] ç¡®ä¿å®ƒæ˜¯ 4D ç»“æ„ (N, A, K, 9)
        # å¦‚æœ C++ è¿”å›çš„æ˜¯æ‰å¹³çš„ 3D (N, A, K*9)ï¼Œæˆ‘ä»¬å…ˆæ¢å¤å®ƒä»¥ä¾¿åç»­å¤„ç†ï¼Œ
        # ä½†æ—¢ç„¶æˆ‘ä»¬æœ€ç»ˆè¦ Flattenï¼Œå…¶å®å¯ä»¥ç›´æ¥å¤„ç†ï¼Œä¸è¿‡ä¸ºäº†é€»è¾‘ç»Ÿä¸€ï¼Œæˆ‘ä»¬å…ˆä¿æŒæ ‡å‡†å½¢çŠ¶ã€‚
        if obs_tensor.ndim == 3:
            N, A, FlatDim = obs_tensor.shape
            max_partners = constants.MAX_PARTNER_COUNT # 149
            feature_dim = FlatDim // max_partners # åº”è¯¥ = 9
            
            try:
                obs_tensor = obs_tensor.view(N, A, max_partners, feature_dim)
            except Exception as e:
                print(f"âŒ Reshape Failed: {e}")

        # [PROBE] æœ€åä¸€æ¬¡ç¡®è®¤ï¼ˆåªæ‰“å°ä¸€æ¬¡ï¼‰
        if not hasattr(self, "_probe_flatten_checked"):
            print(f"\n[PYTHON PROBE] Partner Flatten Check")
            print(f"  > Raw Shape: {obs_tensor.shape}")
            if obs_tensor.shape[-1] == 9:
                 print(f"  > âœ… Feature Dim is 9. Flattening for MLP...")
            self._probe_flatten_checked = True

        # 2. åº”ç”¨æ©ç å¹¶å±•å¹³ (Flatten) ä¸ºç¥ç»ç½‘ç»œè¾“å…¥æ ¼å¼
        if mask is not None:
            # [Case A] æœ‰ Mask: (TotalAgents, K, 9) -> (TotalAgents, K*9)
            # ç»“æœæ˜¯ 2Dï¼Œå¯ä»¥ä¸ EgoState (TotalAgents, 6) æ‹¼æ¥
            return obs_tensor[mask].flatten(start_dim=1)
            
        # [Case B] æ—  Mask: (N, A, K, 9) -> (N, A, K*9)
        # ç»“æœæ˜¯ 3Dï¼Œå¯ä»¥ä¸ EgoState (N, A, 6) æ‹¼æ¥
        return obs_tensor.flatten(start_dim=2)

               


# gpudrive/env/env_torch.py -> _get_road_map_obs å‡½æ•° (æ›¿æ¢æ•´ä¸ªå‡½æ•°ä½“)

    def _get_road_map_obs(self, mask=None):
        """Get road map observations."""
        
        # [GIGAFLOW FIX] C++ ä¾§å·²åˆ é™¤æ­¤ Tensorï¼Œè¿”å›é›¶å¼ é‡ä»¥ç»´æŒç»´åº¦
        road_map_feature_dim = 0
        
        if mask is not None:
            valid_count = mask.sum().item()
            return torch.zeros(
                valid_count, 
                road_map_feature_dim, 
                dtype=torch.float32
            ).to(self.device)
            
        # å¦‚æœæ²¡æœ‰ maskï¼Œè¿”å› (Num_Worlds, Max_Agents, 0)
        return torch.zeros(
            self.num_worlds, 
            self.max_agent_count, 
            road_map_feature_dim, 
            dtype=torch.float32
        ).to(self.device)
    # å¦‚æœé…ç½®è¦æ±‚å¯ç”¨ï¼Œåˆ™è¿”å›é›¶ï¼Œå› ä¸ºé‡å»ºæ•°æ®ä¸é€šè¿‡æ­¤ Tensor å¯¼å‡º
    
    def _get_lidar_obs(self, mask=None):
        """Get lidar observations."""

        if not self.config.lidar_obs:
            return torch.Tensor().to(self.device)

        lidar = LidarObs.from_tensor(
            lidar_tensor=self.sim.lidar_tensor(),
            backend=self.backend,
            device=self.device,
        )

        if mask is not None:
            return [
                lidar.agent_samples[mask],
                lidar.road_edge_samples[mask],
                lidar.road_line_samples[mask],
            ]
        else:
            return torch.cat(
                [
                    lidar.agent_samples,
                    lidar.road_edge_samples,
                    lidar.road_line_samples,
                ],
                dim=-1,
            ).flatten(start_dim=2)

# gpudrive/env/env_torch.py -> _get_bev_obs å‡½æ•° (æ›¿æ¢åŸå‡½æ•°ä½“)

    def _get_bev_obs(self, mask=None):
        """Get BEV segmentation map observation.

        Returns:
            torch.Tensor: (num_worlds, max_agent_count, resolution, resolution, 1)
        """
        # [GIGAFLOW FIX] C++ ä¾§å·²åˆ é™¤æ­¤ Tensorï¼Œè¿”å›ç©ºå¼ é‡
        return torch.Tensor().to(self.device)


    def _get_vbd_obs(self, mask=None):
        """
        Get ego-centric VBD trajectory observations for controlled agents using matrix operations.

        Args:
            mask: Optional mask to filter agents

        Returns:
            Tensor of ego-centric VBD trajectories
        """
        if not self.use_vbd or self.vbd_model is None:
            return torch.Tensor().to(self.device)

        # Get current agent positions and orientations
        agent_state = GlobalEgoState.from_tensor(
            abs_self_obs_tensor=self.sim.absolute_self_observation_tensor(),
            backend=self.backend,
            device=self.device,
        )

        # Initialize output tensor
        traj_feature_dim = (
            self.vbd_trajectories.shape[2] * self.vbd_trajectories.shape[3]
        )

        if mask is not None:
            # Count valid agents for output tensor size
            valid_count = mask.sum().item()
            ego_vbd_trajectories = torch.zeros(
                (valid_count, traj_feature_dim), device=self.device
            )

            # Track which output index we're filling
            out_idx = 0

            # Process each world
            for w in range(self.num_worlds):
                # Get valid agent indices for this world
                world_mask = mask[w]
                agent_indices = torch.where(world_mask)[0]

                if len(agent_indices) == 0:
                    continue

                # Extract ego positions and yaws for these agents
                ego_pos_x = agent_state.pos_x[w, agent_indices]
                ego_pos_y = agent_state.pos_y[w, agent_indices]
                ego_yaw = agent_state.rotation_angle[w, agent_indices]

                # Process each agent in this world
                for i, agent_idx in enumerate(agent_indices):
                    # Get global trajectory for this agent
                    global_traj = self.vbd_trajectories[w, agent_idx]

                    # Create 2D rotation matrix for this agent
                    cos_yaw = torch.cos(ego_yaw[i])
                    sin_yaw = torch.sin(ego_yaw[i])
                    rotation_matrix = torch.tensor(
                        [[cos_yaw, sin_yaw], [-sin_yaw, cos_yaw]],
                        device=self.device,
                    )

                    # Transform positions using matrix multiplication
                    pos_xy = global_traj[:, :2]
                    ego_pos = torch.tensor(
                        [ego_pos_x[i], ego_pos_y[i]], device=self.device
                    ).reshape(1, 2)
                    translated_pos = (
                        pos_xy - ego_pos
                    )  # Broadcasting to subtract from all timesteps
                    rotated_pos = torch.matmul(
                        translated_pos, rotation_matrix.T
                    )

                    # Transform velocities (only rotation, no translation)
                    vel_xy = global_traj[:, 3:5]
                    rotated_vel = torch.matmul(vel_xy, rotation_matrix.T)

                    # Create transformed trajectory
                    transformed_traj = torch.zeros_like(global_traj)
                    transformed_traj[:, :2] = rotated_pos
                    transformed_traj[:, 2] = (
                        global_traj[:, 2] - ego_yaw[i]
                    )  # Adjust heading
                    transformed_traj[:, 3:5] = rotated_vel

                    # Flatten and add to output
                    ego_vbd_trajectories[out_idx] = transformed_traj.reshape(
                        -1
                    )
                    out_idx += 1

            if self.config.norm_obs:
                traj_len = self.vbd_trajectories.shape[2]
                ego_vbd_trajectories = self._normalize_vbd_obs(
                    ego_vbd_trajectories, traj_len
                )

            return ego_vbd_trajectories

        else:
            # Without mask, process all agents in all worlds
            ego_vbd_trajectories = torch.zeros(
                (self.num_worlds, self.max_agent_count, traj_feature_dim),
                device=self.device,
            )

            # Process each world
            for w in range(self.num_worlds):
                # Get controlled agent indices for this world
                valid_mask = self.cont_agent_mask[w]
                world_agent_indices = torch.where(valid_mask)[0]

                if len(world_agent_indices) == 0:
                    continue

                # Extract ego positions and yaws
                ego_pos_x = agent_state.pos_x[w]
                ego_pos_y = agent_state.pos_y[w]
                ego_yaw = agent_state.rotation_angle[w]

                # Process each agent in this world
                for agent_idx in world_agent_indices:
                    # Get global trajectory
                    global_traj = self.vbd_trajectories[w, agent_idx]

                    # Create 2D rotation matrix for this agent
                    cos_yaw = torch.cos(ego_yaw[agent_idx])
                    sin_yaw = torch.sin(ego_yaw[agent_idx])
                    rotation_matrix = torch.tensor(
                        [[cos_yaw, sin_yaw], [-sin_yaw, cos_yaw]],
                        device=self.device,
                    )

                    # Transform positions
                    pos_xy = global_traj[:, :2]
                    ego_pos = torch.tensor(
                        [ego_pos_x[agent_idx], ego_pos_y[agent_idx]],
                        device=self.device,
                    ).reshape(1, 2)
                    translated_pos = pos_xy - ego_pos
                    rotated_pos = torch.matmul(
                        translated_pos, rotation_matrix.T
                    )

                    # Transform velocities
                    vel_xy = global_traj[:, 3:5]
                    rotated_vel = torch.matmul(vel_xy, rotation_matrix.T)

                    # Create transformed trajectory
                    transformed_traj = torch.zeros_like(global_traj)
                    transformed_traj[:, :2] = rotated_pos
                    transformed_traj[:, 2] = (
                        global_traj[:, 2] - ego_yaw[agent_idx]
                    )
                    transformed_traj[:, 3:5] = rotated_vel

                    # Flatten and add to output
                    ego_vbd_trajectories[
                        w, agent_idx
                    ] = transformed_traj.reshape(-1)

            if self.config.norm_obs:
                traj_len = self.vbd_trajectories.shape[2]
                ego_vbd_trajectories = self._normalize_vbd_obs(
                    ego_vbd_trajectories, traj_len
                )

            return ego_vbd_trajectories

    def _normalize_vbd_obs(self, trajectories_flat, traj_len):
        """
        Normalize flattened VBD trajectory values to be between -1 and 1, with clipping.

        Args:
            trajectories_flat: Flattened tensor containing trajectory data
            traj_len: Number of trajectory steps

        Returns:
            Normalized flattened trajectories tensor
        """
        # Get original shape for proper reshaping
        original_shape = trajectories_flat.shape

        # Calculate feature dimension
        feature_dim = 5  # x, y, yaw, vel_x, vel_y

        # Reshape to separate the features
        if len(original_shape) == 2:  # (num_agents, flattened_features)
            traj_features = trajectories_flat.reshape(
                -1, traj_len, feature_dim
            )
        else:  # (num_worlds, max_agents, flattened_features)
            traj_features = trajectories_flat.reshape(
                original_shape[0], original_shape[1], traj_len, feature_dim
            )

        # Normalize each feature
        # x, y positions
        traj_features[..., 0] = normalize_min_max(
            tensor=traj_features[..., 0],
            min_val=constants.MIN_REL_GOAL_COORD,
            max_val=constants.MAX_REL_GOAL_COORD,
        )
        traj_features[..., 1] = normalize_min_max(
            tensor=traj_features[..., 1],
            min_val=constants.MIN_REL_GOAL_COORD,
            max_val=constants.MAX_REL_GOAL_COORD,
        )

        # Normalize yaw angle
        traj_features[..., 2] = (
            traj_features[..., 2] / constants.MAX_ORIENTATION_RAD
        )

        # Normalize velocities
        traj_features[..., 3] = traj_features[..., 3] / constants.MAX_SPEED
        traj_features[..., 4] = traj_features[..., 4] / constants.MAX_SPEED

        # Clip all values to the [-1, 1] range
        traj_features = torch.clamp(traj_features, min=-1.0, max=1.0)

        # Reshape back to original format
        return traj_features.reshape(original_shape)

    # [æ›¿æ¢æ•´ä¸ª get_obs æ–¹æ³•]
    def get_obs(self, mask=None):
        """
        [GIGAFLOW FIXED] Pipeline: Compact Obs (15 dim) -> C++ Reconstruct -> Python Normalize
        Total Output Dim: 925
        """
        # =================================================================
        # 1. å‡†å¤‡æ•°æ®æº (Compact 15-dim)
        # =================================================================
        # è¿™é‡Œçš„ 15 ç»´ç»“æ„å¿…é¡»ä¸ sim.cpp reconstructLogic ä¸­çš„è¯»å–é¡ºåºä¸¥æ ¼ä¸€è‡´
        # Indices 0-7: Self Observation (Speed, Size, Goal, Collision, ID)
        self_obs = self.sim.self_observation_tensor().to_torch()
        
        # Indices 8-14: Absolute Observation (Pos X,Y,Z + Rot X,Y,Z,W)
        # æˆ‘ä»¬åªå–å‰ 7 ç»´ (Pos + Rot)
        abs_obs = self.sim.absolute_self_observation_tensor().to_torch()[..., :7]
        
        # æ‹¼æ¥: (NumWorlds, MaxAgents, 15)
        compact_obs = torch.cat([self_obs, abs_obs], dim=-1)

        # =================================================================
        # 2. å‡†å¤‡ C++ è¾“å…¥ (Flatten & Contiguous)
        # =================================================================
        if mask is not None:
            # Case A: æœ‰æ©ç ï¼Œåªå¤„ç†æœ‰æ•ˆ Agent
            # Shape: (N_valid, 15)
            obs_input = compact_obs[mask]
        else:
            # Case B: æ— æ©ç ï¼Œå¤„ç†æ‰€æœ‰ Agent (ä¿æŒ Batch ç»´åº¦ä»¥ä¾¿åç»­æ¢å¤)
            # Shape: (NumWorlds * MaxAgents, 15)
            obs_input = compact_obs.flatten(0, 1)
            
        # å¿…é¡»ç¡®ä¿å†…å­˜è¿ç»­ï¼Œå¦åˆ™ C++ æŒ‡é’ˆä¼šè¯»åˆ°ä¹±ç 
        if not obs_input.is_contiguous():
            obs_input = obs_input.contiguous()

        # =================================================================
        # 3. è°ƒç”¨ C++ é‡å»º (Zero-Copy)
        # =================================================================
        # è¿™ä¸€æ­¥ä¼šç¬é—´ç”Ÿæˆ 925 ç»´çš„åŸå§‹ç‰©ç†æ•°æ®
        ptr = obs_input.data_ptr()
        rows = obs_input.shape[0]
        cols = obs_input.shape[1] # åº”è¯¥æ˜¯ 15
        
        # è¿”å›: (Rows, 925)
        obs_raw = self.sim.reconstruct_observations(ptr, rows, cols).to_torch()

        # =================================================================
        # =================================================================
        # [æ¢é’ˆ 1] C++ åŸå§‹æ•°æ®æ£€æŸ¥ (Raw Physics Units)
        # =================================================================
        # ä»…åœ¨ World 0 çš„ç¬¬ä¸€æ­¥æ‰“å°ï¼Œé˜²æ­¢åˆ·å±
        if not hasattr(self, "_probe_raw_printed") and obs_raw.shape[0] > 0:
            print(f"\n{'='*20} [PROBE 1] C++ Raw Output (925-dim) {'='*20}")
            print(f"Shape: {obs_raw.shape}")
            
            # å–ç¬¬ä¸€ä¸ª Agent çš„æ•°æ®æ ·æœ¬
            sample = obs_raw[0] 
            
            # A. è‡ªè½¦ (Ego) - åº”è¯¥æ˜¯ç‰©ç†æ•°å€¼ (é€Ÿåº¦ m/s, ä½ç½® m)
            print(f"  ğŸš— Ego (0-5): {sample[0:6].tolist()}")
            print(f"     > Speed: {sample[0]:.2f} (Expected: ~0-30)")
            print(f"     > Size:  {sample[1]:.2f}x{sample[2]:.2f}")
            print(f"     > Goal:  ({sample[3]:.2f}, {sample[4]:.2f})")
            
            # B. é‚»å±… (Partner) - åº”è¯¥åŒ…å«ç›¸å¯¹ä½ç½®
            print(f"  ğŸ‘¥ Partner 0 (6-14): {sample[6:15].tolist()}")
            print(f"     > Rel Pos: ({sample[7]:.2f}, {sample[8]:.2f})")
            
            # C. åœ°å›¾ (Map) - åº”è¯¥åŒ…å«ç›¸å¯¹ä½ç½®å’Œå°ºå¯¸
            # Index 285 æ˜¯ç¬¬ä¸€ä¸ªåœ°å›¾ç‚¹çš„å¼€å§‹ (6 + 31*9)
            map_start = 285
            print(f"  ğŸ›£ï¸ Map Point 0 ({map_start}-{map_start+9}): {sample[map_start:map_start+10].tolist()}")
            print(f"     > Rel Pos: ({sample[map_start]:.2f}, {sample[map_start+1]:.2f})")
            
            # æ•´ä½“ç»Ÿè®¡
            print(f"  ğŸ“Š Stats: Min={obs_raw.min().item():.2f}, Max={obs_raw.max().item():.2f}")
            print("="*60 + "\n")
            self._probe_raw_printed = True
        elif not hasattr(self, "_probe_raw_printed") and obs_raw.shape[0] == 0:
            print(f"[PROBE 1] Skipped: No agents in current pass (Shape: {obs_raw.shape})")
        # =================================================================
        # 4. å½’ä¸€åŒ– (Normalization)
        # =================================================================
        # å°†åŸå§‹ç‰©ç†å•ä½ (ç±³, m/s) è½¬æ¢ä¸ºç¥ç»ç½‘ç»œå‹å¥½çš„ [-1, 1]
        obs_norm = self._normalize_reconstructed_obs(obs_raw)
        # =================================================================
        # [æ¢é’ˆ 2] ç¥ç»ç½‘ç»œè¾“å…¥æ£€æŸ¥ (Normalized Data)
        # =================================================================
        if not hasattr(self, "_probe_norm_printed") and obs_norm.shape[0] > 0:
            print(f"\n{'='*20} [PROBE 2] NN Input (Normalized) {'='*20}")
            
            idx = 0
            sample = obs_norm[15]
            
            # A. è‡ªè½¦ (Ego) - åº”è¯¥åœ¨ [-1, 1] æˆ– [0, 1]
            print(f"  ğŸš— Ego (0-5): {sample[0:6].tolist()}")
            print(f"     > Speed (Norm): {sample[0]:.4f}")
            
            # B. é‚»å±…
            print(f"  ğŸ‘¥ Partner 0 (6-14): {sample[6:15].tolist()}")
            
            # C. åœ°å›¾
            map_start = 285
            print(f"  ğŸ›£ï¸ Map Point 0: {sample[map_start:map_start+10].tolist()}")
            
            # æ•´ä½“ç»Ÿè®¡
            print(f"  ğŸ“Š Stats: Min={obs_norm.min().item():.4f}, Max={obs_norm.max().item():.4f}")
            
            if obs_norm.abs().max() > 5.0:
                print("  âŒ [WARNING] Values > 5.0 detected! Normalization might be wrong.")
            else:
                print("  âœ… [OK] Values are within reasonable range.")
            print("="*60 + "\n")
            self._probe_norm_printed = True
        # =================================================================
        # =================================================================
        # 5. æ¢å¤å½¢çŠ¶ (å¦‚æœéœ€è¦)
        # =================================================================
        if mask is None:
            # (NumWorlds * MaxAgents, 925) -> (NumWorlds, MaxAgents, 925)
            obs_norm = obs_norm.view(self.num_worlds, self.max_agent_count, -1)
            
        return obs_norm

    def _normalize_reconstructed_obs(self, obs_flat):
        """
        [Helper] å¯¹ 925 ç»´æ‰å¹³å‘é‡è¿›è¡Œå½’ä¸€åŒ–
        obs_flat Shape: (N, 925)
        """
        if not self.config.norm_obs:
            return obs_flat

        # Clone ä»¥é¿å…åŸåœ°ä¿®æ”¹å½±å“ç¼“å­˜ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        obs = obs_flat.clone()

        # --- A. Ego State (Indices 0-6) ---
        # 0:Speed, 1:Len, 2:Wid, 3:GoalX, 4:GoalY, 5:Collision
        obs[:, 0] /= constants.MAX_SPEED
        obs[:, 1] /= constants.MAX_VEH_LEN
        obs[:, 2] /= constants.MAX_VEH_WIDTH
        obs[:, 3] = normalize_min_max(obs[:, 3], constants.MIN_REL_GOAL_COORD, constants.MAX_REL_GOAL_COORD)
        obs[:, 4] = normalize_min_max(obs[:, 4], constants.MIN_REL_GOAL_COORD, constants.MAX_REL_GOAL_COORD)
        # Index 5 is Collision (0/1), skip

        # --- B. Partner Obs (Indices 6-285) ---
        # 31 Partners * 9 Features
        num_partners = constants.MAX_PARTNER_COUNT # 31
        feat_dim = constants.PARTNER_FEAT_DIM      # 9
        
        # æå–åˆ‡ç‰‡å¹¶ Reshape ä¸º (N, 31, 9) æ–¹ä¾¿æ‰¹é‡æ“ä½œ
        start_idx = 6
        end_idx = start_idx + (num_partners * feat_dim)
        partners = obs[:, start_idx:end_idx].view(-1, num_partners, feat_dim)
        
        # 0:Speed, 1:PosX, 2:PosY, 3:Heading, 4:Len, 5:Wid ...
        partners[..., 0] /= constants.MAX_SPEED
        partners[..., 1] = normalize_min_max(partners[..., 1], constants.MIN_REL_GOAL_COORD, constants.MAX_REL_GOAL_COORD)
        partners[..., 2] = normalize_min_max(partners[..., 2], constants.MIN_REL_GOAL_COORD, constants.MAX_REL_GOAL_COORD)
        partners[..., 3] /= constants.MAX_ORIENTATION_RAD
        partners[..., 4] /= constants.MAX_VEH_LEN
        partners[..., 5] /= constants.MAX_VEH_WIDTH
        partners[..., 8] /= 20.0
        # å†™å›
        obs[:, start_idx:end_idx] = partners.flatten(1).clone()

        # --- C. Map Obs (Indices 285-925) ---
        # 64 Points * 10 Features
        num_map = constants.MAX_ROAD_OBS_COUNT # 64
        map_dim = constants.ROAD_GRAPH_FEAT_DIM  # 10
        
        map_start = end_idx
        map_end = map_start + (num_map * map_dim)
        road_map = obs[:, map_start:map_end].view(-1, num_map, map_dim)
        
        # 0:PosX, 1:PosY, 2:ScaleX, 3:ScaleY, 4:Heading ...
        road_map[..., 0] = normalize_min_max(road_map[..., 0], constants.MIN_REL_GOAL_COORD, constants.MAX_REL_GOAL_COORD)
        road_map[..., 1] = normalize_min_max(road_map[..., 1], constants.MIN_REL_GOAL_COORD, constants.MAX_REL_GOAL_COORD)
        road_map[..., 2] /= constants.MAX_ROAD_SCALE
        road_map[..., 3] /= constants.MAX_ROAD_SCALE
        road_map[..., 4] /= constants.MAX_ORIENTATION_RAD
        road_map[..., 5] /= 20.0
        road_map[..., 6] = 0.0

        # å†™å›
       

        return obs

    # ä¿®æ”¹æ–‡ä»¶: gpudrive/env/env_torch.py

    def get_controlled_agents_mask(self):
        """
        [GIGAFLOW FIX] è·å–æ§åˆ¶æ©ç ã€‚
        ä¸ä»…è¯»å– C++ çš„çŠ¶æ€ï¼Œè¿˜é€šè¿‡æ£€æŸ¥ç‰©ç†åæ ‡æ¥è¿‡æ»¤æ‰ 'å¹½çµæ™ºèƒ½ä½“'ã€‚
        """
        # 1. è·å– C++ è®¤ä¸ºçš„æ§åˆ¶çŠ¶æ€ (ç›®å‰å®ƒæ˜¯å…¨ 1ï¼Œä¸å‡†ç¡®)
        raw_mask = (
            self.sim.controlled_state_tensor().to_torch().clone() == 1
        ).squeeze(axis=2)

        # 2. [æ–°å¢] ç‰©ç†ä½ç½®æ£€æŸ¥
        # è·å–æ‰€æœ‰æ™ºèƒ½ä½“çš„ X åæ ‡
        # self_observation_tensor å¸ƒå±€: [Speed, Length, Width, GoalX, GoalY, Collision, ID]
        # ç­‰ç­‰ï¼Œæˆ‘ä»¬éœ€è¦ç»å¯¹åæ ‡æ¥åˆ¤æ–­æ˜¯å¦åœ¨ -11000
        abs_obs = self.sim.absolute_self_observation_tensor().to_torch()
        pos_x = abs_obs[:, :, 0] # (NumWorlds, MaxAgents)

        # 3. å®šä¹‰è¿‡æ»¤å™¨ï¼šåªæœ‰åæ ‡å¤§äº -500 çš„æ‰ç®—æ´»äºº
        # (æ­£å¸¸åœ°å›¾åæ ‡æ˜¯ 0~500ï¼Œå¹½çµæ˜¯ -11000)
        valid_physics_mask = (pos_x > -500.0)

        # 4. åˆå¹¶æ©ç ï¼šæ—¢è¦ C++ è¯´å¯æ§ï¼Œåˆè¦ç‰©ç†ä¸Šå­˜åœ¨
        final_mask = raw_mask & valid_physics_mask

        # [å¯é€‰] æ‰“å°ä¸€æ¬¡è¯Šæ–­ä¿¡æ¯ï¼Œç¡®è®¤è¿‡æ»¤ç”Ÿæ•ˆ
        if not hasattr(self, "_mask_debug_printed"):
            total_slots = raw_mask.numel()
            valid_agents = final_mask.sum().item()
            ghosts = total_slots - valid_agents
            print(f"\nğŸ›¡ï¸ [MASK SYSTEM] Ghost Filter Installed.")
            print(f"   > Total Slots: {total_slots}")
            print(f"   > Real Agents: {valid_agents}")
            print(f"   > Ghosts Killed: {ghosts}")
            self._mask_debug_printed = True

        return final_mask

    def advance_sim_with_log_playback(self, init_steps=0):
        """Advances the simulator by stepping the objects with the logged human trajectories.

        Args:
            init_steps (int): Number of warmup steps.
        """
        if init_steps >= self.config.episode_len:
            raise ValueError(
                "The length of the expert trajectory is 91,"
                f"so init_steps = {init_steps} should be < than 91."
            )

        self.init_frames = []

        self.log_playback_traj, _, _, _ = self.get_expert_actions()

        for time_step in range(init_steps):
            self.step_dynamics(
                actions=self.log_playback_traj[:, :, time_step, :]
            )

    def remove_agents_by_id(
        self, perc_to_rmv_per_scene, remove_controlled_agents=True
    ):
        """Delete random agents in scenarios.

        Args:
            perc_to_rmv_per_scene (float): Percentage of agents to remove per scene
            remove_controlled_agents (bool): If True, removes controlled agents. If False, removes uncontrolled agents
        """
        # Obtain agent ids
        agent_ids = LocalEgoState.from_tensor(
            self_obs_tensor=self.sim.self_observation_tensor(),
            backend="torch",
            device=self.device,
        ).id

        # Choose the appropriate mask based on whether we're removing controlled or uncontrolled agents
        if remove_controlled_agents:
            agent_mask = self.cont_agent_mask
        else:
            # Create inverse mask for uncontrolled agents
            agent_mask = ~self.cont_agent_mask

        for env_idx in range(self.num_worlds):
            # Get all relevant agent IDs (controlled or uncontrolled) for the current environment
            scene_agent_ids = agent_ids[env_idx, :][agent_mask[env_idx]].long()

            if (
                scene_agent_ids.numel() > 0
            ):  # Ensure there are agents to sample
                # Determine the number of agents to sample (X% of the total agents)
                num_to_sample = max(
                    1, int(perc_to_rmv_per_scene * scene_agent_ids.size(0))
                )

                # Randomly sample agent IDs to remove using torch
                sampled_indices = torch.randperm(scene_agent_ids.size(0))[
                    :num_to_sample
                ]
                sampled_agent_ids = scene_agent_ids[sampled_indices]

                # Delete the sampled agents from the environment
                self.sim.deleteAgents({env_idx: sampled_agent_ids.tolist()})

        # Reset controlled agent mask and visualizer
        self.cont_agent_mask = self.get_controlled_agents_mask()
        self.max_agent_count = self.cont_agent_mask.shape[1]
        self.num_valid_controlled_agents_across_worlds = (
            self.cont_agent_mask.sum().item()
        )

        # Reset static scenario data for the visualizer
        self.vis.initialize_static_scenario_data(self.cont_agent_mask)

    def swap_data_batch(self, new_sim_scenes: list = None):
        """
        Swap the current scenes with granular memory profiling to isolate the leak.
        """
        import gc
        import torch

        if new_sim_scenes is None:
            raise ValueError("swap_data_batch éœ€è¦ 'new_sim_scenes' å‚æ•°ã€‚")

        # --- [è¾…åŠ©å‡½æ•°] æ˜¾å­˜å¿«ç…§ ---
        def get_gpu_snapshot(tag):
            if not torch.cuda.is_available(): return
            torch.cuda.synchronize()
            gc.collect() # å¼ºåˆ¶GCä»¥æ’é™¤Pythonå¯¹è±¡å¼•ç”¨çš„å¹²æ‰°
            
            # 1. PyTorch è§†è§’
            res = torch.cuda.memory_reserved() / 1024**3
            
            # 2. ç‰©ç†ç¡¬ä»¶è§†è§’ (ç­‰åŒäº nvidia-smi)
            free_mem, total_mem = torch.cuda.mem_get_info()
            physical_used = (total_mem - free_mem) / 1024**3
            
            print(f"  [{tag}] Phys: {physical_used:.4f} GB | PyTorch Rsrv: {res:.4f} GB")
            return physical_used

        print(f"\n====== Resample Memory Diagnosis (Scenes: {len(new_sim_scenes)}) ======")
        baseline_mem = get_gpu_snapshot("0. Start")

        # -----------------------------------------------------------
        # é˜¶æ®µ 1: Python çŠ¶æ€æ›´æ–°
        # -----------------------------------------------------------
        self.sim_scenes = new_sim_scenes
        self.num_worlds = len(self.sim_scenes)
        self.data_batch = None

        if len(self.sim_scenes) != self.num_worlds:
            raise ValueError("Data batch size mismatch")
        
        # -----------------------------------------------------------
        # é˜¶æ®µ 2: C++ æ¨¡æ‹Ÿå™¨é‡ç½® (Manager::setMaps)
        # -----------------------------------------------------------
        # ç†è®ºä¸Šè¿™é‡Œåº”è¯¥é›¶å¢é•¿ï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»åˆ‡æ–­äº† C++ åˆ†é…
        if torch.cuda.is_available(): torch.cuda.synchronize()
        self.sim.set_maps(new_sim_scenes)
        
        mem_after_cpp = get_gpu_snapshot("1. After C++ setMaps")

        # -----------------------------------------------------------
        # é˜¶æ®µ 3: æ©ç æ›´æ–° (çº¯ Python/Tensor æ“ä½œ)
        # -----------------------------------------------------------
        self.cont_agent_mask = self.get_controlled_agents_mask()
        self.max_agent_count = self.cont_agent_mask.shape[1]
        self.num_valid_controlled_agents_across_worlds = self.cont_agent_mask.sum().item()
        
        mem_after_mask = get_gpu_snapshot("2. After Mask Update")

        # -----------------------------------------------------------
        # é˜¶æ®µ 4: VBD æ¨¡å‹æ¨ç† (å¦‚æœæœ‰)
        # -----------------------------------------------------------
        if self.use_vbd and self.vbd_model is not None:
            self._generate_vbd_trajectories()
            mem_after_vbd = get_gpu_snapshot("3. After VBD Gen")
        else:
            mem_after_vbd = mem_after_mask # è·³è¿‡

        # -----------------------------------------------------------
        # é˜¶æ®µ 5: å¯è§†åŒ–å™¨åˆå§‹åŒ– (Matplotlib/Rendering)
        # -----------------------------------------------------------
        # è¿™æ˜¯æœ€å¤§çš„å«Œç–‘å¯¹è±¡
        self.vis.initialize_static_scenario_data(self.cont_agent_mask)
        
        mem_after_vis = get_gpu_snapshot("4. After Vis Init")

        # -----------------------------------------------------------
        # æ€»ç»“æŠ¥å‘Š
        # -----------------------------------------------------------
        delta = mem_after_vis - baseline_mem
        print(f"====== Diagnosis Summary ======")
        print(f"Total Physical Increase: {delta:+.4f} GB")
        
        # ç®€æ˜“å½’å› åˆ†æ
        diff_cpp = mem_after_cpp - baseline_mem
        diff_vbd = mem_after_vbd - mem_after_mask
        diff_vis = mem_after_vis - mem_after_vbd
        
        if diff_cpp > 0.01: print(f"âš ï¸ SUSPECT: C++ Backend (+{diff_cpp:.4f} GB)")
        if diff_vbd > 0.01: print(f"âš ï¸ SUSPECT: VBD Model (+{diff_vbd:.4f} GB)")
        if diff_vis > 0.01: print(f"âš ï¸ SUSPECT: Visualizer (+{diff_vis:.4f} GB)")
        print("===================================================\n")



    def _generate_vbd_trajectories(self):
        """Generate and store trajectory predictions for all scenes using VBD model."""
        if not self.use_vbd or self.vbd_model is None:
            return

        _ = self.reset()

        # Generate sample batch using the limited mask
        sample_batch = self._generate_sample_batch(init_steps=self.init_steps)

        # VBD model prediction
        predictions = self.vbd_model.sample_denoiser(sample_batch)
        vbd_trajectories = (
            predictions["denoised_trajs"].to(self.device).numpy()
        )
        agent_indices = sample_batch["agents_id"]

        self.vbd_trajectories.zero_()
        # Process each world separately
        for world_idx in range(self.num_worlds):
            world_agent_indices = agent_indices[world_idx]

            # Filter out negative indices (they're our padding)
            valid_mask = (
                world_agent_indices >= 0
            )  # Boolean mask of valid indices
            valid_agent_indices = world_agent_indices[
                valid_mask
            ]  # Filtered tensor

            if len(valid_agent_indices) > 0:
                # Update vbd_trajectories(x, y, yaw, vel_x, vel_y) for this world's agents
                self.vbd_trajectories[
                    world_idx, valid_agent_indices, :, :2
                ] = torch.Tensor(
                    vbd_trajectories[
                        world_idx, : len(valid_agent_indices), :, :2
                    ]
                )
                self.vbd_trajectories[
                    world_idx, valid_agent_indices, :, :2
                ] -= self.sim.world_means_tensor().to_torch()[
                    world_idx, :2
                ]  # subtract mean
                self.vbd_trajectories[
                    world_idx, valid_agent_indices, :, 2
                ] = torch.Tensor(
                    vbd_trajectories[
                        world_idx, : len(valid_agent_indices), :, 2
                    ]
                )
                self.vbd_trajectories[
                    world_idx, valid_agent_indices, :, 3:
                ] = torch.Tensor(
                    vbd_trajectories[
                        world_idx, : len(valid_agent_indices), :, 3:5
                    ]
                )

    def get_expert_actions(self):
        """Get expert actions for the full trajectories across worlds.

        Returns:
            expert_actions: Inferred or logged actions for the agents.
            expert_speeds: Speeds from the logged trajectories.
            expert_positions: Positions from the logged trajectories.
            expert_yaws: Heading from the logged trajectories.
        """

        log_trajectory = LogTrajectory.from_tensor(
            self.sim.expert_trajectory_tensor(),
            self.num_worlds,
            self.max_agent_count,
            backend=self.backend,
        )

        if self.config.dynamics_model == "delta_local":
            inferred_actions = log_trajectory.inferred_actions[..., :3]
            inferred_actions[..., 0] = torch.clamp(
                inferred_actions[..., 0], -6, 6
            )
            inferred_actions[..., 1] = torch.clamp(
                inferred_actions[..., 1], -6, 6
            )
            inferred_actions[..., 2] = torch.clamp(
                inferred_actions[..., 2], -torch.pi, torch.pi
            )
        elif self.config.dynamics_model == "state":
            # Extract (x, y, yaw, velocity x, velocity y)
            inferred_actions = torch.cat(
                (
                    log_trajectory.pos_xy,
                    torch.ones(
                        (*log_trajectory.pos_xy.shape[:-1], 1),
                        device=self.device,
                    ),
                    log_trajectory.yaw,
                    log_trajectory.vel_xy,
                    torch.zeros(
                        (*log_trajectory.pos_xy.shape[:-1], 4),
                        device=self.device,
                    ),
                ),
                dim=-1,
            )
        elif (
            self.config.dynamics_model == "classic"
            or self.config.dynamics_model == "bicycle"
        ):
            inferred_actions = log_trajectory.inferred_actions[..., :3]
            inferred_actions[..., 0] = torch.clamp(
                inferred_actions[..., 0], -6, 6
            )
            inferred_actions[..., 1] = torch.clamp(
                inferred_actions[..., 1], -0.3, 0.3
            )

        return (
            inferred_actions,
            log_trajectory.pos_xy,
            log_trajectory.vel_xy,
            log_trajectory.yaw,
        )

    def get_env_filenames(self):
        """Obtain the tfrecord filename for each world, mapping world indices to map names."""

        map_name_integers = self.sim.map_name_tensor().to_torch()
        filenames = {}
        # Iterate through the number of worlds
        for i in range(self.num_worlds):
            tensor = map_name_integers[i]
            # Convert ints to characters, ignoring zeros
            map_name = "".join([chr(i) for i in tensor.tolist() if i != 0])
            filenames[i] = map_name

        return filenames

    def get_scenario_ids(self):
        """Obtain the scenario ID for each world."""
        scenario_id_integers = self.sim.scenario_id_tensor().to_torch()
        scenario_ids = {}

        # Iterate through the number of worlds
        for i in range(self.num_worlds):
            tensor = scenario_id_integers[i]
            # Convert ints to characters, ignoring zeros
            scenario_id = "".join([chr(i) for i in tensor.tolist() if i != 0])
            scenario_ids[i] = scenario_id

        return scenario_ids


if __name__ == "__main__":

    env_config = EnvConfig(
        dynamics_model="delta_local",
    )
    render_config = RenderConfig()

    # --- [ä¿®æ”¹ 9] ç§»é™¤ SceneDataLoader
    # train_loader = SceneDataLoader(
    #     root="data/processed/examples",
    #     batch_size=2,
    #     dataset_size=100,
    #     sample_with_replacement=True,
    #     shuffle=False,
    # )
    
    # --- [ä¿®æ”¹ 10] åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„ sim_scenes åˆ—è¡¨ç”¨äºæµ‹è¯•
    # (!!!) ç¡®ä¿è¿™äº›è·¯å¾„æ˜¯æœ‰æ•ˆçš„ï¼Œå¦åˆ™æµ‹è¯•ä¼šå¤±è´¥ (!!!)
    TEST_MAP_DIR = "data/processed/examples" # å‡è®¾çš„è·¯å¾„
    try:
        # å°è¯•ä»ç›®å½•åŠ¨æ€åŠ è½½
        import glob
        import os
        test_scenes_paths = glob.glob(f"{TEST_MAP_DIR}/scene-*.tfrecord")
        if len(test_scenes_paths) < 2:
             raise FileNotFoundError("æµ‹è¯• tfrecord æ–‡ä»¶ä¸è¶³")
        test_scenes = test_scenes_paths[:2] # å–å‰ä¸¤ä¸ª
        print(f"__main__ æµ‹è¯•: æ‰¾åˆ°åœºæ™¯ {test_scenes}")
    except Exception as e:
        print(f"__main__ æµ‹è¯•: æ— æ³•åŠ è½½æµ‹è¯•åœºæ™¯: {e}ã€‚")
        test_scenes = []

    if len(test_scenes) > 0:
        # --- [ä¿®æ”¹ 11] ä½¿ç”¨æ–°çš„ __init__ ç­¾å
        env = GPUDriveTorchEnv(
            config=env_config,
            sim_scenes=test_scenes, # <--- ä¼ é€’åœºæ™¯
            max_cont_agents=64,
            device="cpu",
        )

        control_mask = env.cont_agent_mask

        # Rollout
        obs = env.reset()

        sim_frames = []
        agent_obs_frames = []

        expert_actions, _, _, _ = env.get_expert_actions()

        env_idx = 0

        for t in range(10):
            print(f"Step: {t}")

            # Step the environment
            expert_actions, _, _, _ = env.get_expert_actions()
            env.step_dynamics(expert_actions[:, :, t, :])

            highlight_agent = torch.where(env.cont_agent_mask[env_idx, :])[0][
                -1
            ].item()

            # Make video
            sim_states = env.vis.plot_simulator_state(
                env_indices=[env_idx],
                zoom_radius=50,
                time_steps=[t],
                center_agent_indices=[highlight_agent],
            )

            agent_obs = env.vis.plot_agent_observation(
                env_idx=env_idx,
                agent_idx=highlight_agent,
                figsize=(10, 10),
            )

            sim_frames.append(img_from_fig(sim_states[0]))
            agent_obs_frames.append(img_from_fig(agent_obs))

            obs = env.get_obs()
            reward = env.get_rewards()
            done = env.get_dones()
            info = env.get_infos()

            if done[0, highlight_agent].bool():
                break

        env.close()

        media.write_video(
            "sim_video.gif", np.array(sim_frames), fps=10, codec="gif"
        )
        media.write_video(
            "obs_video.gif", np.array(agent_obs_frames), fps=10, codec="gif"
        )

# ... (æ–‡ä»¶çš„å…¶ä½™éƒ¨åˆ†ä¿æŒä¸å˜)

# # # =================================================================
# # # [GIGAFLOW DIAGNOSIS] å°†æ­¤ä»£ç å—æ·»åŠ åˆ°æ–‡ä»¶æœ€æœ«å°¾
# # # è¿è¡Œå‘½ä»¤: python gpudrive/env/env_torch.py
# # # =================================================================
# if __name__ == "__main__":
#     import torch
#     import os
#     import glob
#     from gpudrive.env.config import EnvConfig

#     print("ğŸš€ å¯åŠ¨ GIGAFLOW ç¢°æ’/è¶Šé‡è¯Šæ–­ç¨‹åº (åº•å›¾ç‰ˆ)...")

#     # ==========================================
#     # [å…³é”®é…ç½®] è¯·å¡«å…¥ä½ ä»¬é‚£å¼ "åº•å›¾"çš„ç»å¯¹è·¯å¾„
#     # ==========================================
#     # ä¾‹å¦‚: "/root/code/gpudrive/maps/Town01.json"
#     base_map_path = "/root/code/gpudrive/maps/Town01_tessellated.json"  # <--- ä¿®æ”¹è¿™é‡Œï¼
    
#     # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
#     if not os.path.exists(base_map_path):
#         print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°åº•å›¾æ–‡ä»¶: {base_map_path}")
#         print("   è¯·ä¿®æ”¹è„šæœ¬ä¸­çš„ `base_map_path` ä¸ºä½ ä»¬å®é™…ä½¿ç”¨çš„ JSON åœ°å›¾è·¯å¾„ã€‚")
#         print("   C++ éœ€è¦è¯»å–å®ƒæ¥æ„å»ºé“è·¯ç½‘æ ¼ï¼Œå¦åˆ™æ— æ³•æ£€æµ‹è¶Šé‡å’Œç¢°æ’ã€‚")
#         exit(1)

#     # ==========================================
#     # 2. é…ç½®ç¯å¢ƒ (å°†è¿™ä¸€å¼ å›¾å¤åˆ¶ N ä»½)
#     # ==========================================
#     test_num_worlds = 32  
    
#     # è¿™å°±æ˜¯å‘Šè¯‰æ¨¡æ‹Ÿå™¨ï¼š
#     # "æˆ‘æœ‰ 32 ä¸ªä¸–ç•Œï¼Œæ¯ä¸ªä¸–ç•Œéƒ½ä½¿ç”¨è¿™å¼ åº•å›¾ä½œä¸ºç‰©ç†ç¯å¢ƒ"
#     sim_scenes = [base_map_path] * test_num_worlds

#     config = EnvConfig()
#     config.device = "cuda"

#     # ==========================================
#     # 3. åˆå§‹åŒ– & è¿è¡Œè¯Šæ–­
#     # ==========================================
#     try:
#         print(f"âœ… åŠ è½½åº•å›¾: {base_map_path}")
#         print(f"âœ… åˆå§‹åŒ– {test_num_worlds} ä¸ªå¹¶è¡Œä¸–ç•Œ...")
        
#         env = GPUDriveTorchEnv(
#             config=config,
#             sim_scenes=sim_scenes, 
#             max_cont_agents=config.max_num_agents_in_scene, 
#             device="cuda",
#         )
        
#         print("ğŸ”„ æ­£åœ¨é‡ç½®ç¯å¢ƒ (Reset)...")
#         # Reset æ—¶ï¼ŒC++ ä¼šï¼š
#         # 1. åŠ è½½åº•å›¾çš„é“è·¯ (åªåšä¸€æ¬¡)
#         # 2. è°ƒç”¨ level_gen.cpp ç”Ÿæˆæ™ºèƒ½ä½“
#         env.reset()
        
#         print("â–¶ï¸ æ‰§è¡Œ Step 1...")
#         dummy_actions = torch.zeros(
#             (env.num_worlds, env.max_agent_count, 3), 
#             device="cuda"
#         )
#         env.step_dynamics(dummy_actions)
        
#         # [æ–°ä»£ç  - ç›´æ¥è·å–åŸå§‹ Tensor]
#         # ç»•è¿‡ Info ç±»çš„å°è£…ï¼Œç›´æ¥æ‹¿åº•å±‚æ•°æ®
#         infos = env.sim.info_tensor().to_torch()

#         # --- æ ¸å¿ƒè¯Šæ–­é€»è¾‘ ---
#         # è¿‡æ»¤ Type 7 (Vehicle)
#         agent_types = infos[:, :, 4]
#         vehicle_mask = (agent_types == 7)
#         real_infos = infos[vehicle_mask]
        
#         active_agents = vehicle_mask.sum().item()
        
#         if active_agents > 0:
#             collisions = real_infos[:, 1].sum().item()
#             offroad = real_infos[:, 5].sum().item()
#         else:
#             collisions = 0
#             offroad = 0

#         print("\n" + "="*40)
#         print(f"ğŸ“Š === è¯Šæ–­ç»“æœ (Total Agents: {active_agents}) ===")
#         print("="*40)
#         print(f"ğŸ’¥ æ£€æµ‹åˆ°çš„ç¢°æ’ (Collisions): {collisions}")
#         print(f"ğŸšœ æ£€æµ‹åˆ°çš„è¶Šé‡ (Off-road):   {offroad}")
#         print("-" * 40)

#         # ç»“æœåˆ¤å®š
#         if active_agents == 0:
#             print("âš ï¸ è­¦å‘Š: æ²¡æœ‰æ£€æµ‹åˆ° Type=7 çš„æ´»è·ƒæ™ºèƒ½ä½“ï¼")
#             print("   -> è¯·æ£€æŸ¥ level_gen.cpp æ˜¯å¦æ­£ç¡®ç”Ÿæˆäº†æ™ºèƒ½ä½“ã€‚")
            
#         elif collisions > 0 or offroad > 0:
#             print("âœ… [SUCCESS] ç»“è®º: C++ ä¿®å¤å·²ç”Ÿæ•ˆï¼")
#             print(f"   Python æˆåŠŸåŸºäºåº•å›¾ '{os.path.basename(base_map_path)}' è¯»å–åˆ°äº†çŠ¶æ€ã€‚")
#         else:
#             print("âŒ [FAILURE] ç»“è®º: æ•°å€¼ä¾ç„¶ä¸º 0ã€‚")
#             print("   -> å°è¯•è¿ç»­è·‘ 20 æ­¥...")
#             for i in range(20):
#                 env.step_dynamics(dummy_actions)
#                 # [æ–°ä»£ç  - ç›´æ¥è·å–åŸå§‹ Tensor]
#         # ç»•è¿‡ Info ç±»çš„å°è£…ï¼Œç›´æ¥æ‹¿åº•å±‚æ•°æ®
#                 infos = env.sim.info_tensor().to_torch()
#                 real_infos = infos[infos[:, :, 4] == 7]
#                 if len(real_infos) > 0:
#                     c = real_infos[:, 1].sum().item()
#                     o = real_infos[:, 5].sum().item()
#                     if c > 0 or o > 0:
#                         print(f"   Step {i+2}: âœ… ç»ˆäºæ£€æµ‹åˆ°äº†ï¼Collisions={c}, Offroad={o}")
#                         break

#     except Exception as e:
#         print(f"\nâŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
#         import traceback
#         traceback.print_exc()
    
#     finally:
#         if 'env' in locals():
#             env.close()